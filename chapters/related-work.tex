\chapter{Related Work} \label{chap:rw}

The following sections introduce the two main topics dealt with in this dissertation (merge conflicts and the usage of large language models for software verification and test generation). They explore related work and how we can build on it to develop our approach.

\section{Semantic Conflict Detection}

The study of merge techniques and conflicts has a long history, likely even predating the specific terminology itself. Thus there is a large corpus of work to explore.

Several related works exist which seeks to develop methods that can more systemically identify adulterated behaviour arising from semantic conflicts.

\subsection{Detection without Testing}

Several solutions exist which attempt to identify the presence of semantic conflicts without generating unit tests. An example of this rests on verifying overriding assignment: if a merge was successful where the same variable was assigned in both A and B, it's likely this is unintentional and there is a conflict. Thus, the solution checks if such a situation happens and reports if so to the developers~\cite{kn:assigne}.


Static analysis solutions have been considered, implementing the previous overriding assignment checks, inter procedural data flow and program dependence graph, which try to find data and control flows between the changes done by both developers, as well as confluence, which checks if there is data and control flows from both branches that flow into some common path. This static analysis shows much better F1 score and recall than dynamic analysis techniques, but much worse precision~\cite{kn:staticanal}.


The tool DeltaImpactFinder compares the impact of a change in the origin and destination branches, calculating the difference, the ``Delta-Impact''. To measure impact, dependencies are mapped out: the assumption is that when an entity is changed its dependencies are impacted. A semantic conflict is identified if there are missing or extra dependencies and this can be identified with the calculation of the delta-impact~\cite{kn:deltaimpact}.

The notion of semantic conflict-freedom is seen as a sufficient condition for correctness. The SafeMerge~\cite{kn:safemerge} tool seeks to verify the presence of semantic conflict-freedom in the merge process, avoiding the creation of semantic conflicts. Semantic conflict-freedom can be defined, for branches O (Origin), A, B and M (Merge), for all i inputs:
\begin{itemize}
  \item If out(O,i) $\neq$ out(A,i), then out(M,i) = out(A,i)
  \item If out(O,i) $\neq$ out(B,i), then out(M,i) = out(B,i)
  \item Otherwise out(O,i) = out(A,i) = out(B,i) = out(M,i)
\end{itemize}

\subsection{Detection with Testing}

In identifying the presence of semantic conflicts in a merge conflict, developed solutions have focused on the automatic generation of unit tests.

By Da Silva et al., we find an attempt at identifying cases of semantic conflict by applying automated behaviour change detection~\cite{kn:leuson}. In summary, with a base commit B, a left L, right R and merge M, they observe that a generated unit test that passes in L but fails in B partially reveals the effect of the changes made in that branch. If the test then fails in M, it is likely the changes made in R interfere. To generate unit tests they used EvoSuite~\cite{kn:evosuite} and Randoop~\cite{kn:randoop}.

In their analysis they found that the developed tool only detected interference in four out of 15 changes within merge scenarios that do actually suffer from interference, corresponding then to a recall of 0.267. While this is a very modest rate, it displayed no false positives (precision of 1) and thus could likely be integrated in a testing process to prune possible merge conflicts early, or further studied and refined.

 Building upon their previous work, \citet{kn:leuson2} proposed SAM (SemAntic Merge), a tool that generates tests upon merges in Java.\footnote{SAM can be found at \url{https://github.com/leusonmario/SAM}.}. In summary, SAM initially does a simple textual merge to integrate the difference branches while identifying possible textual conflicts. After merging four program versions are built, to fully describe the merge scenario under test: Base, Left, Right, Merge. Source code transformations to improve testability are also part of the process, but optional. Finally, the test generation tools are fed objects serialized during the execution of existing test suites. After applying four test generations tools: EvoSuite~\cite{kn:evosuite}, Differential EvoSuite, Randoop~\cite{kn:randoop} and Randoop Clean, their own adapted version of Randoop, SAM executes the generated tests against the four versions of the program, identifies which tests failed, interpreting it with pre-defined interference criteria heuristics and from there reports conflicts, if detected~\cite{kn:leuson2}.

Detecting nine out of 28 conflicts, it shows improvements over previous work: the authors specifically highlight the best performance when combining tests from only EvoSuite and Differential EvoSuite. Regarding behaviour changes specifically, 89 are found. 
In both cases, they highlight the ability of transformations (for example, making private fields public) to increase testability, showing moderate improvements in some tested scenarios, with 20 additional changes and three additional conflicts detected (Differential Evosuite detects 3, each other generator detect one conflict each).  Regarding the 19 false-negatives, 11 of them showed behaviour changes, which were not caused by the changes due to the semantic conflicts~\cite{kn:leuson2}.


\citet{kn:nuno} has proposed the tool UNSETTLE (aUtomatic uNit teSt gEneraTion for semanTic confLict dEtection)\footnote{UNSETTLE can be found at \url{https://github.com/conflito/unsettle}.} which is composed of two modules:
%
\begin{itemize}
  \item \textbf{Changes-Matcher} module that identifies the possible presence of semantic conflicts, by first computing the changes between different versions (base and variants) and then comparing it to a set of patterns (listed in \Cref{table:pattern-table}) describing common sources of conflicts as a base. From this it generates a DSL file, highlighting which methods and classes should be put under test to identify the conflict.

  \item \textbf{Test generator module}, a modified version of EvoSuite that takes the previously created artifact as an input to guide test generation.
\end{itemize}
%
Of particular interest to us is the Changes-Matcher module, as this is also the starting point for our work, with the usage of a LLM over EvoSuite for test generation instead.

\begin{table}[t]
\setlength\extrarowheight{2pt}
\begin{tabularx}{\textwidth}{llX}
 \toprule
 \textbf{Group} & \textbf{ID} & \textbf{Description} \\
 \midrule
 Change Method  & CM & Update two different dependencies of a method or update one method and concurrently update one of its
dependencies \\ 
 \hline
 Change Method
and Field & CMF & Change the type of one field to a type that does not
override a method while a dependency for the method
is added to a method that reads the field
 \\
 \hline
 Dependency
Based & DB & Update a method while a dependency to it is added
concurrently \\
 \hline
 Field Hiding & FH & Hide the field of a superclass in a subclass and concurrently add a method in the subclass that writes the
super field
 \\
 \hline
 Overload by Access Change & OAC & Change the visibility of an overloaded method and
concurrently add a dependency to it \\ 
 \hline
 Overload by Addition & OA & Overload a method and concurrently add a dependency to it \\  
 \hline
 Parallel Changes & P & Concurrent changes to the same entity, i.e., method
(PM), constructor, (PC) or field (PF) \\ 
 \hline
 Remove Overriding & RO & Remove the override of a method and concurrently
add a dependency to it \\  
  \hline
 Unexpected Overriding & UO & Override a method in a subclass while a dependency to it is added concurrently (AO) or override an
Object-inherited method and concurrently add a dependency to it (UO) \\
 \bottomrule
\end{tabularx}
\caption{\label{table:pattern-table}Semantic Conflict Patterns identified by Changes-Matcher~\cite{kn:nuno}.}
\end{table}


Ti Jao et al have proposed test oracles for program merges. Most significantly, not only do they support two and three-way merges but also octopus merges (merge with any number of branches). The developed tool, TOM (testing on merges), generates tests to identify unexpected and lost behaviour. For this, they implement diff-line as a criterion, guiding the tests to cover lines that have been modified between the different program versions, generating different assertions for different versions. Stability checking is done: the test is rerun five times before being handed to the developers. The tool identifies 45 three-way and 87 octopus merges, from a universe of 389 of each~\cite{kn:ji2022}.

\Cref{table:tool-comparison} shows a comparison between some key features of the presented solutions.


\begin{table}[t]
\setlength\extrarowheight{2pt}
\begin{tabularx}{\textwidth}{p{0.15\textwidth}p{0.17\textwidth}p{0.10\textwidth}p{0.13\textwidth}p{0.305\textwidth}}
  \toprule
  \textbf{Tool} & \textbf{Recall} & \textbf{Octopus Merge Support} & \textbf{Publicly Available} & \textbf{Test Generator} \\
  \midrule
  SAM & 43\% \newline (12 out of 28) & No & Yes & EvoSuite, Differential EvoSuite, Randoop, Randoop Clean \\ \hline
  UNSETTLE & 35\% \newline (6 out of 17) & No & Yes & EvoSuite \\ \hline
  TOM & 17\% \newline (132 out of 778) & Yes & No & EvoSuite \\
 \bottomrule
\end{tabularx}
\caption{\label{table:tool-comparison}Comparison of SAM~\cite{kn:leuson2}, UNSETTLE~\cite{kn:nuno}, and TOM tools~\cite{kn:ji2022}.}
\end{table}


% 12 out of 28 SAM
% 132 out of 778 TOM
% changes matcher always finds it, tests are 6 out of 17

\subsubsection{Conflict definition to assess tests}\label{chap:rw:behaviours}

To assess whether a conflict is correctly identified with unit tests, we need to select a definition that relates to testing.
Thus, two concepts are usually considered: emergent behaviour and lost behaviour. Respectively this refers to behaviour that 
is introduced in the process of merging, despite not being present in either branch, and behaviour that exists in one branch
but is then lost in the merge. While several existing works have established formal definitions for these behaviours~\cite{kn:taoji,kn:leuson},
we focus on the definition formulated by~\citet{kn:nuno}, as they broadened previous formulations to consider cases where the test does not apply,
ie, cases where the test does not compile. This is common when a conflict includes a method that was introduced in one branch: a test generated including
it will compile for that branch and merge, but not the other branch or base. A case of this is the example presented in the introduction \Cref{fig:conflict}: to test the emergent behaviour we would need to test checkout, and this test is not applicable for base and branch A.

Thus, the behaviours are defined as:

\textbf{Emergent Behaviour}: Let \texttt{t} be a successful test for the merged program \texttt{M} with \texttt{$V_{1}$}
and \texttt{$V_{2}$} as its parents. We then say that \texttt{M} has emergent behaviour if \texttt{t} applies to both
parents and fails over the same assertion \texttt{$\psi$} in both, or \texttt{t} applies to a single parent and fails.

\textbf{Lost Behaviour}: Suppose that the parent versions \texttt{$V_{1}$} and \texttt{$V_{2}$} have the common ancestor \texttt{B}.
Let \texttt{t} be a successful test case for \texttt{$V_{i}$}.
If \texttt{t} applies to \texttt{B} and \texttt{M} and \texttt{t} fails in both over the same assertion \texttt{$\psi$}, or \texttt{t} does not apply to \texttt{B} and fails in \texttt{M},
or \texttt{t} does not apply to \texttt{B} and \texttt{M}, then we say that the behaviour introduced by \texttt{$V_{i}$} was lost.


\section{Test Generation}

\subsection{Traditional Automatic Test Generation}

Traditional methods of automated test generation can be divided into random and search-based techniques. The former, being random, is simpler and faster, while the latter employs heuristics and search algorithms to fulfil some criteria, like maximizing code coverage.

Randoop is one such example of random test generation. Some of the disadvantages of this technique can be mitigated with the employment of feedback-direction~\cite{kn:randoop}. Effectively, the search space of possible random tests is pruned, by guiding the test generator towards valid cases, avoiding expansion on invalid test cases~\cite{kn:randoop}.

EvoSuite implements search-based automatic test generation for Java and Python code, aiming for tests that achieve high coverage, while being as small as possible and providing assertions. To achieve this they implement both evolutionary search to evolve the suite with respect to a coverage criterion and mutation testing to generate assertions~\cite{kn:evosuite}.

EvoSuiteR extends EvoSuite, aiming to provide automated generation of regression tests. Thus it takes account two versions of the software and aside from coverage, it considers state distance (``how different is the state of
all objects in the test suite across the two versions'') and control flow distance (``how
far are the two versions from diverging'')~\cite{kn:evosuiter}.

The symbolic execution technique, while introduced in the 70s has been more explored as computing power increased. It executes programs with symbolic rather than concrete values: assignments are represented as functions and conditionals as constraints. From this, we can identify specific paths and branches. This can be done statically, we symbolic execution techniques first used to derive all paths or dynamically, with symbolic execution being updated throughout execution, allowing us to find alternatives to the current branch~\cite{kn:symbolicexec}.


Similar suites have been developed, for example, for Python~\cite{kn:pynguin}.

\subsection{Test Generation with LLMs}

The recent explosion in complexity and popularity of LLMs has suscitated developer interest in their abilities to accelerate and automate software engineering. Angela Fan et al. identified that by 2023 3\% of pre-prints were related to Large Language Models and 11\% of those related to their use in software engineering~\cite{kn:angela}. Particularly relevant is their ability to generate tests, with an expectation that they could achieve better coverage, correctness and readability than previous techniques of automated test generation~\cite{kn:junjiewang}.
In comparison to traditional suites for automated test generation, such as EvoSuite, Palus, Randoop, and JTExpert, ChatGPT has shown, given right tuning of temperature settings, to evidentiate equivalent robustness~\cite{kn:gptunitbra}.
From surveys done on the topic, we find Codex and GPT variants to be the most commonly used LLMs for this issue~\cite{kn:junjiewang}.

While many studies suggest LLMs are competitive with traditional methods of software generation, we can also find evidence to the contrary. Specifically, testing across several LLMs, including Codex, StarCoder and GPT-3.5-Turbo, shows they fail in all regards compared to EvoSuite, primarily due to the generation of non-compilable code, often due to ``hallucination'' of non-existent types and methods~\cite{kn:siddiq2023empirical}. Yutian Tang and others find EvoSuite outperforms ChatGPT in code coverage~\cite{kn:tang2023chatgpt}. Given the novelty of the technology, it is unsurprising there is such variance in reported results and research, but it is worth investigating. Research on Copilot, GitHub's solution based on Codex, show interesting results: while 92.45\% of tests generated without an existing testing suite either failed, were broken or empty, by providing an existing suite the figure would drop to 54.72\%~\cite{kn:githubcopilot}, suggesting that providing this information and a structure to work off is extremely valuable, despite still wielding extremely high failure rates.

Despite variable conclusions regarding correctness and coverage, it seems generally agreed that ChatGPT is good at generating readable code. Yutian Tang et al. highlight that despite minor style errors, primarily in indentation, the generated code is clear and easy to understand~\cite{kn:tang2023chatgpt}. A survey of software developers has found ChatGPT to have comparable and even better readability than manually written tests~\cite{kn:chattester}.


Given the LLM can produce unreliable results, the maximization of ChatGPT's abilities regarding test generation has been explored: techniques such as prompting the LLM for an explanation of what the code is intending to do~\cite{kn:nuances} and feeding error messages from codes that fail to compile or execute as intended back to the LLM for correction~\cite{kn:chattester} have shown an amazing capacity for test generation, given the right prompting.

Regarding prompting, understanding how to best engineer prompts to achieve the desired output from the LLM is a particularly relevant issue. For example, in the generation of test inputs, research found ``generate diverse test input'' to be preferable over ``generate test inputs that result in different outputs between PUT and reference versions'', as ChatGPT could not accurately identify the nuances required to correctly carry out the latter's instructions~\cite{kn:nuances}.
With unit testing generation in particular, Zhiqiang Yuan et al., in developing the ChatTester tool, highlight the importance of combining a natural language descriptor with the appropriate code context~\cite{kn:chattester}.

\begin{prompt}
public class Travis {
public static final String TRAVIS NAME = ""travis-ci"";
public static final String TRAVIS = ""TRAVIS"";
public static final String TRAVIS JOB_ID = \"TRAVIS_JOB_ID\";
public static final String TRAVIS PULL_REQUEST = \"TRAVIS_PULL_REQUEST\";
public Travis (final Map<String,String> env);
public boolean isSelected();
public String getName();
public String getJobId();
public String getBranch();
public String getPull Request();

//Focal Method
public Properties getEnvironment() {
  Properties environment new Properties();
  addProperty (environment, ""travis_job_id"", getProperty (TRAVIS_JOB_ID));
  addProperty (environment, "travis_pull request", getProperty(TRAVIS PULL REQUEST));
  return environment;
  }
}

You are a professional who writes Java test methods;
Please write a test method for the method "getEnvironment()" based on the given information using Junit4.
\end{prompt}

Another technique utilized is asking the LLM to infer the intention before developing the tests, which leads it to generate correct tests where basic prompting would fail~\cite{kn:chattester}.

\begin{prompt}
Intention Prompt for ChatGPT

// Focal class
public class StrBuilder {
  public StrBuilder();

  // Focal method
  public StrBuilder setCharAt(final int index, final char c){
    if(index < 0 || index >=length()) {
      throw new StringIndexOutOfBoundsException(index);
    }
    buffer[index] = ch; return this;
  }
}

Please infer the intention of the "setCharAt(final int, final char)".
\end{prompt}

\begin{response}
The intention of 'setCharAt(final int index, final char c)' in the 'StrBuilder' class is to set the character at the specified index in the character buffer to the given character.
\end{response}

\begin{prompt}
Generation Prompt for ChatGPT

// Method Intention
The intention of 'setCharAt(final int index, final char c)' in the 'StrBuilder' class is to set the character at the specified index in the character buffer to the given character.

You are a professional who writes Java test methods.
Please write a test method for the "setCharAt(final int, final char)" with the given Method intention.
\end{prompt}

While they argue that it is little known how effective these strategies are, \citet{kn:johnny} and others highlight several prompting strategies: give examples of desired interaction, write prompts that look somewhat like code and repeat yourself. They note difficulties participants have in prompt generation, such as avoiding giving examples due to fear the LLM will simply replicate it, difficulty in searching online for help and adapting existing solutions and overgeneralizing from a single example~\cite{kn:johnny}.

In prompting we can distinguish a few types of prompts: zero-shot, where instructions are simply given, few-shot, where examples of inputs and outputs are given and few-shot with preamble, combining the previous two examples to give both a preamble instructing the LLM on what to achieve, follow by examples of inputs and outputs~\cite{kn:promptprofiannaca}

Given the natural language aspect of LLM prompting is one of the features that most distinguishes it from classical programming, it is worth exploring methods of systematizing it, making it work more like code. It is with this objective that LMQL (Language Model Query Language) was developed, introducing a scripting based query language~\cite{kn:lmql}. Evidence shows it reduces computing costs by up to 80\%~\cite{kn:lmql}.

The team behind ChatUniTest presents a two-step prompting system, where instructions of what is to be done and how are first provided~\cite{kn:chatunitest}:

\begin{prompt}
Setup Prompt
Please help me generate a JUnit test for a specific Java method using JUnit5 and Mockito3.
I will provide the source code for the method, relevant method signatures and fields, required dependencies, Java class containing the method, expected behaviour, and involved classes in the project.
Create a test that imports necessary dependencies, compiles without errors, and achieves maximum branch and line coverage. 
No explanations needed. 

Specific Test Information
The focal method is ... in the class ..., and the class information is .... The brief information of involved class ... is ...   
\end{prompt}


The training of LLMs in open source repositories gives cause to the fear that automatic test generation might be reliant on the model being trained in the code under test, or simply replicating existing testing suites. To assuage this, it was identified that even when generating tests for packages hosted in GitLab (which were not used to train the LLM under study, gpt3.5-turbo), they still maintained high coverage. In addition, it was found that 60.0\% of the tests had <= 40\% similarity to existing tests and 92.8\% had <= 50\%. Thus we can conclude the LLM is not simply copying tests from the training set without alteration~\cite{kn:max}.


\section{Summary}

The related work aids our own work in several ways: the work explored as regards unit test generation shows us a baseline functioning of the idea we are trying to implement. Particularly, we can use their statistics regarding percentage of conflicts found to assess whether our own solution is an improvement with regard to previous work.

Crucially important too is the work of \citet{kn:nuno} and, as previously mentioned, the developed Changes-Matcher.

The study of works on LLM test generation should also prove useful in the future, allowing us to get a better understanding of their functioning and capabilities, as well as providing ideas and prompt generation techniques which we may have to apply in the future.
