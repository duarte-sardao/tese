    \chapter{Experimental results}\label{chap:results}

This chapters answers \textbf{RQ1 â€“ RQ3}, which study ChatGPT's ability to identify and generate tests for semantic conflict, the latter
with both our prompt, which provides an explanation, and state-of-the-art prompts, which don't.

\section{RQ1}\label{sec:results:rq1}

\begin{table}[t]
\centering
% \tabcolsep=1cm
% \renewcommand{\arraystretch}{0.90}
\begin{tabular}{@{\extracolsep{\fill}} lrrrrr} \toprule
                 & A [0-3] & B [0-1] & C [0-2] & D [0-3] & E [0-3] \\
\midrule
Point (CM)       & 3.0 & 1.0 & 0.7 & 1.0 & 0.7 \\
Fabricated (RO)  & 3.0 & 1.0 & 1.0 & 2.0 & 0.7 \\
Fabricated (OAC) & 2.3 & 1.0 & 1.0 & 3.0 & 0.7 \\
Cart (CM)        & 3.0 & 1.0 & 2.0 & 3.0 & 2.7 \\
Cart (PF)        & 2.3 & 1.0 & 0.0 & 0.0 & 0.0 \\
Cart (PM)        & 2.0 & 1.0 & 1.3 & 1.0 & 0.3 \\
Antlr (PF)       & 1.7 & 1.0 & 0.0 & 0.0 & 0.0 \\
OkHttp (PM)      & 3.0 & 1.0 & 0.7 & 0.0 & 0.0 \\
Retrofit (PM/CM) & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
\midrule
\textit{Average}  & 2.4 & 1.0 & 0.9 & 1.1 & 0.6 \\
\bottomrule
\end{tabular}
\caption{Average values per subject and metric.\label{tab:results:rq1}}
\end{table}

\Cref{tab:results:rq1} summarizes the metrics achieved per subject. From these we can note that:

\begin{itemize}
  \item \textbf{ChatGPT is generally able to identify changes made between branches} ($A=2.4$).

  \item \textbf{In none of the subject did the tool mistake the kind of conflict under discussion} ($B=1.0$). This might be because the prompt did not assert the existence of a conflict, allowing negative responses, whereas other prompts, which asked to find a conflict that did exist, lead the model to find any explanation, which often consisted of describing a textual conflict.

  \item \textbf{Conflict assertion remains uncertain}. ($C=0.9$) In many cases, particularly those of PF type, no conflict as identified. In many other cases it was found but with uncertainty, e.g., with words such as ``possible'', and ``might''.

  \item \textbf{On average, it can identify what originates conflicts} ($D=1.1$).  Despite the range given, values given to the metric were either 0 or 3. This indicates to us in nearly half the cases, the model failed to produce any accurate description, but when it did, it was generally very accurate.

  \item \textbf{ChatGPT struggles with accurately describing the results of semantic conflicts} ($E=0.8$), i.e., how the behaviour is specifically altered and how the outputs change. Indeed, even when the origin of the conflict is found, an accurate description of the results does not necessarily follow, with the topic being vague or not touched upon.
\end{itemize}

The complete prompts, with results and evaluated metrics, can be found in the attachments.
An important observation to make is the variability of results, which highlights the importance of running three trials for our evaluation.
For example, the point class had high variation: two of the results failed to identify any conflict at all, but one of the three not only identified, but quite accurately describe its origin and results.

In the more complex fabricated examples, the biggest issue was the identification of the effects of the semantic conflict, namely that when an overriden function was removed or had its access changed, the other function, with different behaviour, would be used instead. However, given the constraints of the prompt, which is limited to one class, the necessary information could not be transmitted and the LLM argued there would be compilation errors. 
Better results were observed when doing more manual, ad-hoc experiments, as we could identify exactly all the classes involved in the conflict and offer all the required information.

As we proceeded testing the ``Cart'' scenario, we observed a reversal of a trend of very low results for the E metric, particularly for CM and PM.
Indeed for these examples, not only did the LLM identify the origins of the conflict, but exactly the result of this, such as:
``Now, in the merged version, both changes are included. So, when an admin user proceeds to checkout (assuming the admin check in version A evaluates to true), the cost will be set to 0. But immediately after, the checkout method checks if the cost is 0 and throws an exception if it is. This leads to a contradictory behavior: the system sets the cost to 0 for admin users but then doesn't allow them to proceed with checkout due to the cost being 0.''
It is possible this is due to the very straighforward logic of the CM example in particular. What is surprising is that due to some minor changes in the Cart class, we re-ran these tests and originally these values were much lower. Why there was such an improvement is unknown but it is evidence of the variability of these tools.

In a scenario where detection failed we see an example of the LLM providing a grossly wrong explanation: ``If the user is an admin and a discount code that results in a cost of 0 is applied, the exception will never be thrown because the `checkout` method will not be called.''. As regards the conflict itself, this was only completely wrong assertion of its kind: in general where answers failed they were either vague or omitted, not present. The only other major example of a factually incorrect answer came with OkHttp:
\begin{lstlisting}
### Explanation:
The conflict arises because both changes affect the same field (`hostnameVerifier`), but they prescribe different default values.
    
In the merge commit, the decision was made to adopt the change from Version B, which explicitly sets the default `hostnameVerifier` to `OkHostnameVerifier()`. This decision potentially overrides any behavior or configuration related to the default `hostnameVerifier` set in Version A.
    
If the intended behavior was to preserve the default `hostnameVerifier` as defined in Version A, then this would be considered a semantic conflict, as the behavior of the merged version may differ from the behavior of either Version A or Version B individually.
\end{lstlisting}
In this case, the changes made in only one branch are described as a conflict.

Another case of wrong information given by ChatGPT regarded the changes made, both in Cart(PF), which 
correctly describes the removal but incorrectly counts:
\begin{lstlisting}
    1. **Diff Version A**:
    - It removes two discount codes, `"superpromo"` and `"winner"`.
    - This version retains 6 discount codes.
 
 2. **Diff Version B**:
    - It removes one discount code, `"winner"`.
    - This version also retains 6 discount codes.
\end{lstlisting}
As well as Antlr, which fails to accurately describe the changes made:
\begin{lstlisting}
1. **Diff A vs Base:**
    - In version A, the `reduce` keyword was replaced with `return` in the `python2Keywords` array.
 
2. **Diff B vs Base:**
    - In version B, the `del` keyword was added to the `python2Keywords` array.
-------------------------------------------------------------------------------
1. **Diff A vs. Base**:
    - Addition of `"return"` keyword in the `python2Keywords` array.
    - No conflicting changes with other methods or properties.
 
2. **Diff B vs. Base**:
    - Replacement of `"delattr"` with `"del"` in the `python2Keywords` array.
    - No conflicting changes with other methods or properties.
\end{lstlisting}
We observed both of these are ``Parallel Field'' semantic conflicts so it is possible the fact that the changes are being made to fields may have some relation to its inability to express them correctly. 
Further credence to this involves Cart (PM), where edits to the field of an auxiliary class also were misdescribed.
In contrast stands Retrofit: while ChatGPT reliably failed to correctly describe the changes, no edits were made to fields. From observation, we can say these errors may be due
to the fact that the changes are similar and repeated across two complex methods, for branch B. We still see major misdescriptions for branch A, which is relatively simple, with a simple addition of an append to a string.
We see examples of aspects of branch B being assigned to it, and then those same aspects not acknowledged for branch B.
\begin{lstlisting}
    1. In `buildRelativeUrl()`:
    - Version A adds a null check for path parameters and throws an `IllegalArgumentException` if any of them are null. This ensures that path parameters are not null, which could lead to unexpected behavior or errors.
    - Version B also adds a null check for path parameters but handles it differently. Instead of throwing an exception, it continues execution. This could lead to inconsistencies if the application logic relies on path parameters being non-null.
 
 2. In `buildBody()`:
    - Both versions A and B have added null checks for the body parameter in the `SIMPLE` case. This ensures that the body is not null before proceeding with further processing. However, the behavior is consistent between the two versions. 
\end{lstlisting}

Like with Antlr and Retrofit, OkHttp generated no conclusive answers. In particular it tended to offer advice on how identify and diagnose semantic conflicts and make good merges. This is a behaviour that was also observed with other complex, real-world conflicts, during the period of ad-hoc testing.
\begin{lstlisting}
### Potential Semantic Conflict:
While there is no direct semantic conflict, there could be conflicts in the usage and interpretation of the `transports` field and its associated methods. For instance:
- How are these transports selected and utilized within the class?
- Do they introduce any performance or compatibility issues?
- How do these changes impact existing functionality and client behavior?
---------------------------------------------------------------------------------------
### Semantic Conflict Assessment:
Looking at the changes introduced in versions A and B, there does not seem to be a direct semantic conflict between them. Version A primarily deals with transport configuration and introduces a new field `transports` along with related methods. On the other hand, version B modifies the hostname verifier behavior.

However, when merging both changes, we need to ensure that the introduced hostname verifier (`OkHostnameVerifier`) is consistent with the overall configuration of the `OkHttpClient`. Specifically, we need to verify whether `OkHostnameVerifier` is compatible with the behavior and expectations of the existing codebase.
\end{lstlisting}

Evaluating the results as a whole, it is worth imagining in which way the types of conflicts affect the model's capabilities. 
As observed earlier, it seem that Parallel Field conflicts induce errors in branch change description. They also fail to identify the conflict but this seems unrelated to failures of both description:
both cases with succesful and failing descriptions failed to produce conflict identification. Indeed, this was the case, we would expect some description of a conflict, with the same imprecisions, whereas
for our PF cases no conflict was even considered to exist. Our hypothesis is that the conflict is far too simple to be detected: in both cases some type of collection is altered, either with insertions or removals.
The merging of these alterations leads to emergent behaviour: the sets and lists returned are different, with different sizes, when compared to both branches.
However while the behaviour is emergent, it is both expected and likely desirable, hence why it is probably not being seen as conflict.

It is difficult to assess to what extent other types of conflicts affect detection ability. If we observe PM and CM, Cart (PM) and Point (CM) are comparable, while Cart (CM) has near perfect scores and OkHttp (PM) very low ones.
It seems by far that the complexity of the conflict is the determining factor and this complexity can be noted as the complexity of the whole class under test (with OkHttp being the clear outlier), but also in the intricacies of the conflict.
We observe Cart (CM) is fairly straighforward: if the user is an admin, checkout throws and exception. Indeed, even without any of the merge info, it would be fairly trivial to identify this is unintended, whereas
Point and Cart (PM) require the identification of the original behaviour and how it has changed.

It is also worth pointing out that despite good results for metric D in Remove Override and Overload by Access Change, with a perfect 3 in the latter, explanations of the result was flawed, due to a lack of information, ChatGPT reliably understood that trying a function that was removed or that had its access changed would produce errors. Thus it is worth noting in this case, accurate identification of the conflict's result would require a prompt that provides further information.

Overall, we can say ChatGPT can indeed identify and explain whether there is a semantic conflict in a merge. This comes with many caveats however, firstly being that identification does not necessarily lead to a proper explanation. This is not a particularly pressing issue, as a tool that identifies conflicts and then alerts humans, who can themselves describe and fix the issue would already be a great boon for efficient software development.

The bigger caveat is the situations in which it identifies the conflicts: simple fabricated scenarios with significant alterations to behaviour. When the emergent behaviour is small and expected, such as those of PF, the LLM does not view it as a conflict, indicating a bias to clearly unexpected and unintended behaviour. On the other side, if the class is of significant complexity, it becomes harder for the LLM to extract valuable information.
We see these issues combined in OkHttp as the conflict is straighforward: parallel changes result in an object that is different from both branches, thus emergent. Simultaneously, the class is significantly more complex than the other subjects. Thus while in the PF examples, the possibility of a conflict is flat out dismissed,
two of the answers for OkHttp accept the possibility, arguing that more study needs to be done on transports and the hostname verifier respectively.

Further work should thus tackle this: on one hand exploring how to provide a better understanding of semantic conflicts, specifically that emergent behaviour is the result of semantic conflict, even if it does not introduce an apparent bug, software vault or unexpected behaviour.
On the other hand, real-world scenario with complex dependencies, large classes with varied functions, inheritance and other features complicates detection and more complex prompts, data extraction and presentation are likely required.

\section{RQ2}\label{sec:results:rq2}

\Cref{tab:results:rq2} summarizes the results achieved, split by conflict and evaluation type.
These list the passing tests, after edits, the average edit distance to tests that compile and pass in merge 
and the errors present that stop the conflict from being detected.

\Cref{tab:results:rq2tests} gives the values for the tests, after compilation errors are fixed in merge.
These come in the format Base A B Merge. C stands for compilation failures, R for runtime errors, F for failing test and P for passing tests.
In some cases there are several listing per trial, due to the existence of several unit tests.
The first two results for Cart (CM) ChatGPT are italicized: these utilized deprecated features of Junit, and required rolling back to an earlier
version to run.

OkHttp is omitted from the results, as the prompt caused it to only generate imports. It is unknown why.

From this we can observe:
-The main obstacle to the detection of the conflict was incorrect asserts, present in nearly every case.
-In non-compiling tests, those generated with explanations given by ChatGPT tend to require more correction.
-Tests are generally compilable or very close to succesful compilation, with constructor and other errors, such as usage of non-existent getters as the main issues.

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} lrrr} \toprule
                             & Successes (of 3) & Average Edit Distance & Errors \\
    \midrule
    Point (CM) Manual        & 3 & 4 & Assert \\
    Point (CM) ChatGPT       & 3 & 4 & Assert \\
    Fabricated (OAC) Manual  & 3 & 2 & Assert/Branch/Other \\
    Fabricated (OAC) ChatGPT & 3 & 4 & Assert/Branch \\
    Fabricated (RO) Manual   & 1 & 3 & Assert/Miss \\
    Fabricated (RO) ChatGPT  & 1 & 3 & Assert/Miss \\
    Cart (CM) Manual         & 3 & 68 & Assert/Constructor \\
    Cart (CM) ChatGPT        & 3 & 34 & Constructor/Other \\
    Cart (PF) Manual         & 3 & 2 & Assert \\
    Cart (PM) Manual         & 0 & 18 & Other \\
    Cart (PM) ChatGPT        & 1 & 42 & Assert/Other \\
    Antlr (PF) Manual        & 3 & 1 & Assert \\
    Retrofit (PM/CM) Manual  & 0 & N/A & Other \\
    \midrule
    \end{tabular}
    \caption{Results by conflict and explanation type.\label{tab:results:rq2}}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} llcccccccccccccc} 
        \toprule
        Prompt & & \multicolumn{14}{c}{Trial} \\ 
        \cmidrule(lr){3-16}
        & & \multicolumn{4}{c}{1} & \multicolumn{6}{c}{2} & \multicolumn{4}{c}{3} \\ 
                            & & b & A & B & M & & b & A & B & M & & b & A & B & M \\ 
\midrule
Point (CM) Manual           & & F & F & F & P & & F & F & F & P & & F & F & F & P \\ 
Point (CM) ChatGPT          & & F & F & F & P & & F & F & F & P & & F & F & F & P \\ 
                            & & F & F & P & P & & F & F & P & P & & F & F & P & P \\ 
Fabricated (OAC) Manual     & & C & C & F & P & & C & C & F & P & & C & C & F & P \\ 
Fabricated (OAC) ChatGPT    & & C & C & F & P & & C & C & F & P & & C & C & F & P \\ 
Fabricated (RO) Manual      & & C & F & C & P & & C & P & C & P & & C & P & C & P \\ 
Fabricated (RO) ChatGPT     & & C & F & C & P & & C & P & C & P & & C & P & C & P \\
Cart (CM) Manual            & & F & F & F & P & & F & F & F & P & & F & F & F & P \\ 
                            & &   &   &   &   & & P & P & P & P & &   &   &   &   \\ 
Cart (CM) ChatGPT           & & \textit{F} & \textit{F} & \textit{F} & \textit{P} & & \textit{F} & \textit{F} & \textit{F} & \textit{P} & & F & F & F & P \\ %italicize first two
                            & &   &   &   &   & &   &   &   &   & & F & F & P & P \\ 
Cart (PF) Manual            & & F & F & F & P & & F & F & F & P & & F & F & F & P \\ 
                            & & F & F & F & P & & F & F & F & P & & F & F & F & P \\ 
Cart (PM) Manual            & & C & C & P & P & & C & C & P & P & & C & C & P & P \\ 
                            & & P & P & P & P & & C & C & P & P & & P & P & P & P \\ 
                            & &   &   &   &   & & C & C & P & P & & C & C & P & P \\ 
                            & &   &   &   &   & & C & C & P & P & & P & P & P & P \\ 
Cart (PM) ChatGPT           & & C & C & P & P & & C & C & P & P & & C & C & P & P \\
                            & & C & C & P & P & & C & C & F & P & & P & P & P & P \\
                            & &   &   &   &   & & C & C & P & P & & C & C & P & P \\
                            & &   &   &   &   & & C & C & F & P & & P & P & P & P \\
Antlr (PF) Manual           & & F & F & F & P & & F & F & F & P & & F & F & F & P \\ 
        \bottomrule
    \end{tabular}
    \caption{Test running results in base A B Merge \label{tab:results:rq2tests}}
\end{table}


Analysis of \Cref{tab:results:rq2tests} allows us to identify which cases identify the conflict, and which did not.
It is worth examining the results and what they mean, starting by the failing examples:

-PPPP: The test always passes, due to the correct testing of a feature not affected by the merge.

-FFPP and CCPP: The test passes in branch B and in merge. This indicates a behaviour was introduced and remained unaltered in the merge, thus it is not a conflict in itself.
This was common for Point (CM) ChatGPT, as the prompt induced the generation of tests for distance(), which correctly calculated and asserted the euclidean distance of a given point.
They however failed to do the same when applying the distance to move(), thus those required correction.

-CPCP: The same as the previous example, but the change introduced is in branch A.

The examples that show a semantic conflict is present require failure in all brances except merge, or compilation failure in base and one of the branches and failure on the other.

These are:

-FFFP
-CCFP
-CFCP

These all show that behaviour is emergent: something which was never true in any of the previous branches becomes true. The behaviour being asserted is thus the 
emergent behaviour that occurs in merge.

If we observe the results, particularly the errors found, we can see that the main issue present
is incorrect assertions. While in some cases these are understandable (such as the case of RO,
where the values to verify are hash and thus hard to know without running the code), most of the time
they were the result of simple logical or mathematical errors, things that are trivial for a human to get right
and thus also correct.
It is hard to identify why exactly it finds it so hard to get the right asserts. If we observe the cart example,
we find that tests generated for distance() were always correct. These consisted of a simple x + y addition.
However, tests generated for move() were always wrong; in turn these were just x + distance() and y + distance() additions.
Given that it shows it can correctly calculate distance() it is hard to fathom why it struggles
to use it in addition: we can perhaps suggest there is making the association between the method call and the returned value it represents.

Ocasional compilation errors occurred due to the generation of methods that are not present in the code, such as getUser().
In one case, we find an interesting error: the codebase contains methods named ```setResgistration'''.
For the most part they are ignored, as they are not necessary to identify the conflict. In one trial, however,
the method is called for Producer. However the ```Resgistration''' typo is corrected: ``producer.setRegistrationNumber(123);''.
Naturally, this stops code from compiling, but it is a noteworthy example of how Large-Language Models, due to their nature as 
predictive models, can fail in outlier cases, such as an unexpected typo in method namings.

It is noticeable that for Cart(CM), the manual explanation test cases required relatively more edit, while the reverse
is observed for Cart(PM). An explanation might be that one type of prompt generates more complex and longer tests. However as can be seen in
\cref{tab:results:rq2tests}, both examples have comparable amount of test cases and test length does not vary much.
It is possible these are the results of outliers: that if we for example observe Cart (PM), ChatGPT explanation, the results are significantly weighed by
trial 3, which requires significant correction due to the way it tries to access the user:
\begin{response}
    @Test
    public void testSumItemsWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        assertEquals(-50, cart.sum_items(), 0.001);
    }

    @Test
    public void testSumItemsWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        assertEquals(50, cart.sum_items(), 0.001);
    }

    @Test
    public void testTotalCostWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }
\end{response}
has an edit distance of 121, has correction requires extracting the user for access:
\begin{response}
    @Test
    public void testSumItemsWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        assertEquals(0, cart.sum_items(), 0.001);
    }

    @Test
    public void testSumItemsWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        assertEquals(0, cart.sum_items(), 0.001);
    }

    @Test
    public void testTotalCostWithCoupon() {
        User user = new User();
        Cart cart = new Cart(user);
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(0, user.balance, 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        User user = new User();
        Cart cart = new Cart(user);
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(0, user.balance, 0.001);
    }
\end{response}
This same error of trying to access user from cart can be found in other ChatGPT results in PM, but it is ambiguous
whether this is somehow the result of the prompt or coincidental generations. On the other side, Cart (CM) tended to generate
asserts of type AssertThrow and AssertNotThrow, which often had to be reversed to be in line with merge, for manual, whereas for
ChatGPT explanation the conflict these asserts were more often correct. This is clearly an issue with the manually written explanation,
which claims the error throwing behaviour in merge is not intended, leading for tests that assert that nothing is thrown.
It is thus likely good practice to not make value judgements on the behaviour of the code in merge.

On the topic of compilation errors, it is worth nothing often, for Cart(CM) these were related to constructors, as the constructors
for the other classes involved in the Cart conflicts (Item and User) were not included in the prompt. It is interesting
to observe what the common error is: the introduction of extra parameters. For example, in the code of Cart only the fields
value and amount of Item are accessed. However, Item was frequently iniatilized with a constructor with 3 parameter: Name,
Value and Amount. It is fair to assume the Item class would both have a name field and that it would come first in the constructor
and it is likely the training data had many examples of this. Thus it is worth pointing out that testability of code by LLM's can be
improved by closely following norms and rule of thumb expectations and corollarily LLMs will struggle to effectively construct tests
for more unusual pieces of software.

A way to resolve this issue would be to include constructor headers in the original prompt. However one issue is the selection of which
headers: if instead of just Item and User, the Cart class had 20 other dependencies which bore no relevance in our conflict, we would be 
providing extra information in the prompt which could cause confusion. A possibly better option is feeding compilation messages back for correction,
as shown by ChatTester~\cite{kn:chattester}, possibly with the addition of corresponding headers. This corrective approach could also be utilized
to fix assert errors. In effect, it would be an automated way to do the manual corrections we carried out.

Some other issues with constructors we observed are not so obvious to solve: namely that despite knowing of available construcotrs, there's no guarantee
the correct one for the situation will be used. We can see the case of Cart (PM): due to changes in the Item class, constructor information came included
in the diff section of the prompt. This led to the creation of tests using (Value, Coupon) and (Value) format.
\begin{response}
    public class CartTest {

    @Test
    public void testSumItemsWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        assertEquals(-50, cart.sum_items(), 0.001);
    }

    @Test
    public void testSumItemsWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        assertEquals(50, cart.sum_items(), 0.001);
    }

    @Test
    public void testTotalCostWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }
}
\end{response}
Notably these constructors don't set the amount field, which is thus 0, rendering the items useless for any value calculations.
Rectification would require understanding this logic error and thus is liable to be more difficult to achieve. It is for these representations
we have so few successes for Cart (PM): while we corrected the asserts to get passing tests in merge (in this case, setting them to 0, as no
value is subtracted), these corrections don't aid in identifying the conflict.

The other case where the conflict is rarely identified despite our corrections is RO, as it often generated asserts which always passed:
\begin{response}
    @Test
    public void testHashString() {
        InstalacaoAssentos instalacao = new InstalacaoAssentos();
        assertNotNull(instalacao.hashString());
    }
\end{response}
Within the parameters of our work and a corrective system that works off compilation and test errors, this is the most problematic case: as the test works perfectly in merge,
there is nothing prompting us for repair and repair would require more than simply changing a value, but to actually change the type of assert itself.

Retrofit stands as an outlier and indeed we did correct the generated tests as they are so deviated from correctness, any edit would have to fundamentally alter the setup and structure of the test,
to a point where if henceforth the test worked, we could not say it reflected anything on ChatGPT's capabilities. 
From observing, it is clear the LLM in a way does not have enough information to correctly generate tests and thus opts for generating non-existent mocks:
\begin{response}
    @Test(expected = IllegalArgumentException.class)
    public void testBuildRelativeUrlWithNullPathParam() throws UnsupportedEncodingException {
      RequestBuilder builder = new RequestBuilder(mockConverter());
      builder.methodInfo(mockMethodInfo());
      builder.apiUrl("http://example.com");
      builder.args(new Object[]{null}); // Pass null argument
      builder.build(); // This should throw IllegalArgumentException
    }
\end{response}
As well as extensive incomplete test suites, which end up being almost all the same.
\begin{response}
    @Test(expected = IllegalArgumentException.class)
    public void testBuildRelativeUrl_NullPathParam() throws UnsupportedEncodingException {
      RequestBuilder builder = new RequestBuilder(mock(Converter.class));
      builder.methodInfo(mock(RestMethodInfo.class))
             .apiUrl("http://example.com")
             .args(new Object[] {null})
             .build();
    }
  
    @Test(expected = IllegalArgumentException.class)
    public void testBuildBody_NullBody() {
      RequestBuilder builder = new RequestBuilder(mock(Converter.class));
      builder.methodInfo(mock(RestMethodInfo.class))
             .apiUrl("http://example.com")
             .args(new Object[] {null})
             .build();
    }
  
    @Test(expected = IllegalArgumentException.class)
    public void testBuildBody_NullFormUrlEncodedField() {
      RequestBuilder builder = new RequestBuilder(mock(Converter.class));
      builder.methodInfo(mock(RestMethodInfo.class))
             .apiUrl("http://example.com")
             .args(new Object[] {null})
             .build();
    }
  
    @Test(expected = IllegalArgumentException.class)
    public void testBuildBody_NullMultipartPart() {
      RequestBuilder builder = new RequestBuilder(mock(Converter.class));
      builder.methodInfo(mock(RestMethodInfo.class))
             .apiUrl("http://example.com")
             .args(new Object[] {null})
             .build();
    }
\end{response}
It is worth noting none of the tests seem to attempt to test both modified behaviours at once (? addition to query parameters and null handlings). Thus even if correctly designed, they would not detect the conflict.
However this could always be a prompt issue.

It is worth nothing Retrofit has a large existing test suite, including a Helper class which is very helpful for setting up the RequestBuilder with appropriate arguments.
Exploratory work had shown that feeding this test class and prompting for generation on its style could lead to the generation of working (but not detecting) tests. It is probable then,
that for these complex real-world scenarions, aiding test generation by giving examples of existing test suites will be a necessary part of the prompt. Indeed, even if ChatGPT could generate
working tests regardless, it would still be useful for developers to have them more closely conform the existing suite.

Despite all the problems mentioned previously, the results are generally quite good, as most errors are relatively small and easily fixable with further prompting or manual edits as we did.
Thus we can say with some certainty that for these simple cases, most of which where ChatGPT can identify the conflict with some degree of confidence, it can also generate appropriate unit tests.
It is unfortunate that test generation for OkHttp failed so badly was to not even generate tests, as that class was far more intricate and could give evidence to issues such as dealing with
private methods. But it has to be understood that these sort of issues are a possibility given the volatility of LLMs.

\section{RQ3}\label{sec:results:rq3}


%\begin{table}[t]
%    \centering
%    \begin{tabular}{@{\extracolsep{\fill}} lrr} \toprule
%                     & Test Results & Possible Identification \\
%    \midrule
%    Point (CM)       & Passing & No \\
%    Fabricated (RO)  & Failing & Yes \\
%    Fabricated (OAC) & Non-Compiling & Yes \\
%    Cart (CM)        & Non-Compiling & No \\
%    Cart (PF)        & Passing & Yes \\
%    Cart (PM)        & Non-Compiling & Yes \\
%    Antlr (PF)       & Non-Compiling & No \\
%    OkHttp (PM)      & Non-Compiling & No \\
%    Retrofit (PM/CM) & Non-Compiling & No \\
%    %\midrule
%    \bottomrule
%    \end{tabular}
%    \caption{Results for Prompt 1.\label{tab:results:rq3p1}}
%\end{table}

%\begin{table}[t]
%    \centering
%    \begin{tabular}{@{\extracolsep{\fill}} lrr} \toprule
%                     & Test Results & Possible Identification \\
%    \midrule
%    Point (CM)       & Failing & Yes \\
%    Fabricated (RO)  & Passing & No \\
%    Fabricated (OAC) & Failing & Yes \\
%    Cart (CM)        & Non-Compiling & Yes \\
%    Cart (PF)        & Non-Compiling & No \\
%    Cart (PM)        & Non-Compiling & Yes \\
%    Antlr (PF)       & Non-Compiling & No \\
%    OkHttp (PM)      & Non-Compiling & No \\
%    Retrofit (PM/CM) & Non-Compiling & No \\
%    %\midrule
%    \bottomrule
%    \end{tabular}
%    \caption{Results for Prompt 2.\label{tab:results:rq3p2}}
%    \end{table}

%\begin{table}[t]
%    \centering
%    \begin{tabular}{@{\extracolsep{\fill}} lrr} \toprule
%                     & Test Results & Possible Identification \\
%    \midrule
%    Point (CM)       & Runtime & No \\
%    Fabricated (RO)  & Failing & No \\
%    Fabricated (OAC) & Non-Compiling & No \\
%    Cart (CM)        & Non-Compiling & No \\
%    Cart (PF)        & Passing & Yes \\
%    Cart (PM)        & Non-Compiling & Yes \\
%    Antlr (PF)       & Non-Compiling & Yes \\
%    OkHttp (PM)      & Non-Compiling & \todo{hostnameVerifier} \\
%    Retrofit (PM/CM) & Non-Compiling & No \\
%    %\midrule
%    \bottomrule
%    \end{tabular}
%    \caption{Results for Prompt 3.\label{tab:results:rq3p3}}
%    \end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} l*{6}{r}} \toprule
                      & \multicolumn{2}{c}{Prompt 1} & \multicolumn{2}{c}{Prompt 2} & \multicolumn{2}{c}{Prompt 3} \\
                      & Compilation & Passing & Compilation & Passing & Compilation & Passing \\
    \midrule
    Point (CM)       & 3 & 2 & 1 & 0 & 0 & 0 & 1 & 1 \\
    Fabricated (RO)  & 1 & 0 & 1 & 1 & 3 & 0 & 1 & 0 \\ 
    Fabricated (OAC) & 2 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    Cart (CM)        & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    Cart (PF)        & 2 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
    Cart (PM)        & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
    Antlr (PF)       & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    OkHttp (PM)      & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
    Retrofit (PM/CM) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \todo{Trials}    & 3 & 3 & 1 & 1 & 3 & 3 & 1 & 1 \\
    \bottomrule
    \end{tabular}
    \caption{Results for Prompt 1, 2, and 3.\label{tab:results:rq3t1}}
\end{table}
%compilation/runtime

\Cref{tab:results:rq3t1} allows us to observe that issues relating to compilation and correct assertions remain
a problem for these prompts, confirming this is not necessarily just a issue with the prompt we developed. 

Particular prominent issues like the issue of incorrect constructors, such as in the case of Item, remained relevant
and more prevalent as more test cases were generated for each prompt. 

Some new issues with the usage of non-existent methods arose. Whereas before these largely happened with the classes that were not present
in the prompt, we observed the adaptation of the add_item(Item) method into one that took an list input, in Prompt 3:
\begin{response}
    @Before
    public void setup() {
        user = new User("John Doe");
        cart = new Cart(user);
        cartItems = Arrays.asList(new Item("Item1", 10.0, 2, false),
                                  new Item("Item2", 20.0, 1, true));
        cart.add_items(cartItems);
    }
\end{response}
It is possible the reason for this is that the prompt only shows the method signature, which makes it less impactful and thus the LLM may be
more prone to adapt or use it incorrectly. In this case, it is likely a method to add items in this manner was very common on the training dataset,
which goes in line with our previous assessments that issues often arise when our implementation deviates from what is common or expected.

Of note is the case of Prompt 3: while it prompted the LLM to use mocks and reflection when possible, it used these features
for all cases, even though these were unnecessary for the vast majority of cases. It did however generate one passing test for OkHttp,
which so far we had failed to do and where other prompts failed too, as they all sought to access the private method copyWithDefaults().
Of note, however, the 2 compilation failures for OkHttp in prompt 3 also dealt with private access, but to a field rather than a method.

Prompt 4 suffered from limitations of generation and it would often stop generating midtest. For our purposes these were cropped and not counted for the analysis.
This limitation makes sense, as Copilot is primarily targeted at helping developers complete code, as an auxiliary tool, not as a tool to generate very large
code blocks without developer interference.

While we have several passing tests, few are useful in accurately identifying the conflict.
These were Cart (PF) for Prompt 1 as well as Cart (PM) for Prompt 3.


\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} lrrrr} \toprule
                     & Prompt 1 & Prompt 2 & Prompt 3 & Prompt 4 \\
    \midrule
    Point (CM)       & 1 & 1 & 0 & 1 \\
    Fabricated (RO)  & 1 & 0 & 0 & 1 \\
    Fabricated (OAC) & 3 & 1 & 0 & 1 \\
    Cart (CM)        & 0 & 1 & 0 & 0 \\
    Cart (PF)        & 3 & 1 & 1 & 1 \\
    Cart (PM)        & 2 & 0 & 2 & 1 \\
    Antlr (PF)       & 1 & 1 & 0 & 1 \\
    OkHttp (PM)      & 0 & 0 & 0 & 0 \\
    Retrofit (PM/CM) & 0 & 0 & 0 & 0 \\
    Trials           & 3 & 1 & 3 & 1 \\
    %\midrule
    \bottomrule
    \end{tabular}
    \caption{Ammendable tests.\label{tab:results:rq3t2}}
\end{table}

With \Cref{tab:results:rq3t2} we can observe that by applying the same correction process we carried out in
RQ2, we can achieve a meaningful number of tests that not only pass, but also identify the semantic conflict.

It is however notable that these are fewer in number, except maybe Prompt 4, which can be corrected for 6 subjects.
While we find failures for the same reason as in RQ2, where after correction tests pass, but do not identify the conflict,
the characteristics of these prompts also introduced new issues.

One of these relates to Change Method type conflicts: given Prompt 1 and 3 only carry method signatures for non-focal methods,
the relationship between the affected methods is hidden. Thus, for example, no test is generated for Cart (CM) that investigates
checkout with an admin type user as there is not even information that users can have that characteristic. In other cases, it either
calls or mocks methods to ascertain the appropriate assert. This is the case with Point, for example for Prompt 1:
\begin{response}
    @Test
    public void testMove() {
        Point point = new Point(3, 4); // Create a Point object at coordinates (3,4)
        
        double initialDistance = point.distance(); // Calculate initial distance from origin
        point.move(); // Move the point
        
        // Assert that the new coordinates are equal to the initial coordinates plus the initial distance
        assertEquals(3 + initialDistance, point.getX(), 0.0001); 
        assertEquals(4 + initialDistance, point.getY(), 0.0001);
    }
\end{response}
As well as for Prompt 3:
\begin{response}
    @Test
    public void testMove() throws Exception {
        // Mocking distance method
        when(point.distance()).thenReturn(5.0);

        // Using reflection to invoke private method move()
        Point.class.getDeclaredMethod("move").invoke(point);

        // Verifying the new values of x and y after calling move()
        assertEquals(5.0, point.getX(), 0.0);
        assertEquals(5.0, point.getY(), 0.0);
    }
\end{response}
The former does not identify the conflict because the assert, rather than being fixed on the merge behaviour,
varies depending on the result of distance. The latter does not identify it because the behaviour of distance is 
obscured.

Prompt 3 specifically suffer from this degree of excessive mocking when not necessary, either obscuring the behaviour of 
the actual code under test or generating tests that cannot be corrected easily, without reworking the test in general.

The most significant difference, if we dismiss the specific case of Prompt 3, is that while we can achieve succesful results,
there is much higher variability: in most cases for RQ2, if one trial could be ammended, all 3 could be, while here we observe much
higher variability. For example, Prompt 1, for Antlr, generates:
\begin{response}
    @Test
    public void testGetBadWords() {
        Python2Target python2Target = new Python2Target(new CodeGenerator());
        Set<String> badWords = python2Target.getBadWords();

        assertNotNull(badWords);
        assertFalse(badWords.isEmpty());

        // Check some known bad words
        assertTrue(badWords.contains("print"));
        assertTrue(badWords.contains("execfile"));
        assertTrue(badWords.contains("lambda"));

        // Since the set is populated, call getBadWords() again should return the same set
        Set<String> badWordsSecondCall = python2Target.getBadWords();
        assertEquals(badWords, badWordsSecondCall);
    }
\end{response}
This is a perfectly valid test and it verifies several importants features of the getBadWords method: it should
return a non-null, non-empty set, that contains some words we know and should be constant. However to identify the semantic
conflict we need to check the size.

A way to offset this higher variability is simply to generate more tests. Indeed, Prompt 2 specifically indicates the number of
tests to generate, which we specified at 10 as was done before. While it did not always generate the right number, it generated many
tests and often the correct one for our purposes came later. For Cart (CM), the test we want is the 4th:
\begin{response}
    class CartTest {

    private Cart cart;
    private User user;

    @BeforeEach
    void setUp() {
        user = new User("John", false, 100.0);
        cart = new Cart(user);
    }

    @Test
    void testCheckoutWithValidCode() {
        Item item = new Item("Item1", 10.0, 2);
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(80.0, user.balance);
    }

    @Test
    void testCheckoutWithInvalidCode() {
        Item item = new Item("Item2", 20.0, 1);
        cart.add_item(item);
        assertThrows(NullPointerException.class, () -> cart.checkout("invalid"));
    }

    @Test
    void testCheckoutWithZeroCost() {
        Item item = new Item("Item3", 30.0, 1);
        cart.add_item(item);
        assertThrows(RuntimeException.class, () -> cart.checkout("30ff"));
    }

    @Test
    void testCheckoutWithAdminUser() {
        user.setAdmin(true);
        Item item = new Item("Item4", 40.0, 1);
        cart.add_item(item);
        cart.checkout("superpromo");
        assertEquals(100.0, user.balance);
    }

    @Test
    void testCheckoutWithMultipleItems() {
        Item item1 = new Item("Item5", 10.0, 2);
        Item item2 = new Item("Item6", 20.0, 1);
        cart.add_item(item1);
        cart.add_item(item2);
        cart.checkout("2024");
        assertEquals(50.0, user.balance);
    }

    @Test
    void testCheckoutWithEmptyCart() {
        assertThrows(RuntimeException.class, () -> cart.checkout("superpromo"));
    }

    @Test
    void testSumItems() {
        Item item1 = new Item("Item7", 10.0, 2);
        Item item2 = new Item("Item8", 20.0, 1);
        cart.add_item(item1);
        cart.add_item(item2);
        assertEquals(40.0, cart.sum_items());
    }

    @Test
    void testTotalCost() {
        Item item = new Item("Item9", 10.0, 2);
        cart.add_item(item);
        assertEquals(18.0, cart.total_cost(0.1));
    }

    @Test
    void testTotalCostWithAdminUser() {
        user.setAdmin(true);
        Item item = new Item("Item10", 20.0, 1);
        cart.add_item(item);
        assertEquals(0.0, cart.total_cost(0.25));
    }
}
\end{response}

By increasing the number of tests generated we increase the odds of generating the test we are looking for,
but these come at higher costs: more tokens need to be generated, more time is spent generating and a lot more
correction is required. The monetary and time costs are thus much higher.

Prompt 4 also indicated the generation of 10 tests, but Copilot showed to be limited in how much it would generate at once,
leading to less tests per generation. Furthermore tests showed less variance, with variation being mostly on values, not the
test logic. For example, for the same subject:
\begin{response}
    @Test
    void testCheckout() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(990, user.balance);
    }

    @Test
    void testCheckout2() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("superpromo");
        assertEquals(985, user.balance);
    }

    @Test
    void testCheckout3() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("30ff");
        assertEquals(980, user.balance);
    }

    @Test
    void testCheckout4() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("2024");
        assertEquals(990, user.balance);
    }

    @Test
    void testCheckout5() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("invalid");
        assertEquals(1000, user.balance);
    }

    @Test
    void testCheckout6() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(990, user.balance);
    }

    @Test
    void testCheckout7() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
\end{response}
It is worth nothing for this comparison we closely followed Prompt 2's format for 4, with the main variable being the usage of Copilot
instead of ChatGPT. While in this case the metrics between these two were comparable, we have to note that better results, particularly 
in regards to compilation, might have been achieved if we had some adopted some known good practices for Copilot generation, such as giving
context of existing tests \citet{kn:githubcopilot}. But it is also true that test context might also be applicable to our ChatGPT prompts,
after adaptation. The downside is this requires existing tests, which should usually be the case but not always.

In general we verify then that yes, while existing prompts can produce similar results to the prompt developed for RQ2
and can in some cases test the emergent behaviour, this comes with higher randomness and thus associated effort.
By prompting with the explanation of the conflict provided, we can achieve positive results more reliably as the
LLM is given the exact information required on what behaviour needs to be verified.
