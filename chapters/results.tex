\chapter{Experimental results}\label{chap:results}

This chapters answers \textbf{RQ1 -- RQ3}, which aim to study ChatGPT's ability to identify, explain, and reveal semantic conflicts. % the latter with both our prompt, which provides an explanation, and state-of-the-art prompts, which do not.

\section{RQ1: Can ChatGPT identify the presence of a semantic conflict, as well as explain it?}\label{sec:results:rq1}

\begin{table}[t]
\centering
% \tabcolsep=1cm
% \renewcommand{\arraystretch}{0.90}
\begin{tabular}{@{\extracolsep{\fill}} lrrrrr} \toprule
                 & A [0-3] & B [0-1] & C [0-2] & D [0-3] & E [0-3] \\
\midrule
Point (CM)       & 3.0 & 1.0 & 0.7 & 1.0 & 0.7 \\
Fabricated (RO)  & 3.0 & 1.0 & 1.0 & 2.0 & 0.7 \\
Fabricated (OAC) & 2.3 & 1.0 & 1.0 & 3.0 & 0.7 \\
Cart (CM)        & 3.0 & 1.0 & 2.0 & 3.0 & 2.7 \\
Cart (PF)        & 2.3 & 1.0 & 0.0 & 0.0 & 0.0 \\
Cart (PM)        & 2.0 & 1.0 & 1.3 & 1.0 & 0.3 \\
Antlr (PF)       & 1.7 & 1.0 & 0.0 & 0.0 & 0.0 \\
OkHttp (PM)      & 3.0 & 1.0 & 0.7 & 0.0 & 0.0 \\
Retrofit (PM/CM) & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
\midrule
\textit{Average}  & 2.4 & 1.0 & 0.9 & 1.1 & 0.6 \\
\bottomrule
\end{tabular}
\caption{Average values (with corresponding ranges) per subject and metric.\label{tab:results:rq1}}
\end{table}

\Cref{tab:results:rq1} summarizes the metrics evaluated per subject. From these we can note that:

\begin{itemize}
  \item Overall, \textbf{ChatGPT identifies changes made between branches} ($A=2.4$).

  \item \textbf{In none of the trials did the tool attempt to describe a textual conflict} ($B=1.0$). \\
  This might be because the prompt did not assert the existence of a conflict, allowing negative responses, whereas other prompts, which asked to find a conflict that did exist, lead the model to find any explanation, which often consisted of describing a textual conflict.

  \item \textbf{Conflict assertion remains uncertain} ($C=0.9$). \\
  In 13 cases, particularly those of PF type, the response claimed no conflict existed. In five other cases, it was found but with uncertainty, e.g., with words such as ``possible'', and ``might''.

  \item \textbf{On average, it can identify what originates conflicts} ($D=1.1$).  \\
  Despite the range given, values given to the metric were either 0 or 3. This indicates to us in nearly half the cases, the model failed to produce any accurate description, but when it did, it was generally very accurate.

  \item \textbf{ChatGPT struggles with accurately describing the results of semantic conflicts} ($E=0.6$), i.e., how the behaviour is specifically altered and how the outputs change. \\
  Indeed, even when the origin of the conflict is found, an accurate description of the results does not necessarily follow, with the topic being vague or not touched upon.
\end{itemize}

%The complete prompts, with results and evaluated metrics, can be found in the appendix \todo{X}.

\subsection{Conflict Identification}

Initially just evaluating if ChatGPT can recognize the presence of a semantic conflict, we note that
the results are highly variable, which highlights the importance of running three trials for our evaluation.
For example, the Point class had high variation: two of the results failed to identify any conflict at all (despite being so simple), but one of the three not only identified, but quite accurately describe its origin and results.
This remained a general trend, resulting on the average value of 1.0 being predominant for metric C.

It is however worth adressing the exceptions: Cart (CM), which always yields a positive response and Cart (PF) as well as Antlr (PF), which always yield negative responses.
Immediately we can observe that both purely negative examples are of type ``Parallel Field''. When we analyze their responses, we see a pattern: the changes made to the branches 
are incorrectly described.

Cart(PF), incorrectly counts and describes the removal:
\begin{response}
    1. **Diff Version A**:
    - It removes two discount codes, `"superpromo"` and `"winner"`.
    - This version retains 6 discount codes.

 2. **Diff Version B**:
    - It removes one discount code, `"winner"`.
    - This version also retains 6 discount codes.
\end{response}

While Antlr, fails to accurately describe the changes made:
\begin{response}
1. **Diff A vs Base:**
    - In version A, the `reduce` keyword was replaced with `return` in the `python2Keywords` array.

2. **Diff B vs Base:**
    - In version B, the `del` keyword was added to the `python2Keywords` array.
-------------------------------------------------------------------------------
1. **Diff A vs. Base**:
    - Addition of `"return"` keyword in the `python2Keywords` array.
    - No conflicting changes with other methods or properties.

2. **Diff B vs. Base**:
    - Replacement of `"delattr"` with `"del"` in the `python2Keywords` array.
    - No conflicting changes with other methods or properties.
\end{response}
In both cases, the changes are being made to fields, which may have some relation to its inability to express them correctly.
Further credence to this involves Cart (PM), where edits to the field of an auxiliary class were also misdescribed.

In contrast stands Retrofit: while ChatGPT reliably failed to correctly describe the changes, no edits were made to fields. From observation, we can say these errors may be due
to the fact that the changes are similar and repeated across two complex methods, for branch B. We still see major misdescriptions for branch A, which is relatively simple, with a simple addition of an append to a string.
We see examples of changes made in branch B being describe as changes made in A, such as null checks.
\begin{response}
    1. In `buildRelativeUrl()`:
    - Version A adds a null check for path parameters and throws an `IllegalArgumentException` if any of them are null. This ensures that path parameters are not null, which could lead to unexpected behavior or errors.
    - Version B also adds a null check for path parameters but handles it differently. Instead of throwing an exception, it continues execution. This could lead to inconsistencies if the application logic relies on path parameters being non-null.

 2. In `buildBody()`:
    - Both versions A and B have added null checks for the body parameter in the `SIMPLE` case. This ensures that the body is not null before proceeding with further processing. However, the behavior is consistent between the two versions.
\end{response}

While it possible these imprecisions in describing the changes made between branches influence the consideration that no conflict exists, there are other explanations, particularly for the ``Parallel Field'' examples.
The LLM, despite incorrectly listing the changes, shows understanding of the merge done:
\begin{response}
Based on these changes, there isn't a semantic conflict in this merge. The changes from both versions `A` and `B` are independent and do not affect the same lines of code or properties. They both remove different discount codes, and the merge incorporates both of these changes correctly, resulting in a version that has those two discount codes removed.

So, the merge is successful and preserves the intended changes from both versions without causing any conflicts in the functionality of the class.
\end{response}
Our hypothesis is that the conflict is far too simple to be detected: in both cases some type of collection is altered, either with insertions or removals.
The merging of these alterations leads to emergent behaviour: the sets and lists returned are different, with different sizes, when compared to both branches.
However while the behaviour is emergent, it is both expected and likely desirable, hence why it is probably not being seen as conflict.

In contrast to these scenarios, which may be so simple ChatGPT does not understand them as conflicts, we find subjects so complicated, the LLM struggles to responds.
We can observe this with OkHttp, which generated no conclusive answers. In particular it tended to offer advice on how identify and diagnose semantic conflicts and make good merges. This is a behaviour that was also observed with other complex, real-world conflicts, during the period of ad-hoc testing.
\begin{response}
### Potential Semantic Conflict:
While there is no direct semantic conflict, there could be conflicts in the usage and interpretation of the `transports` field and its associated methods. For instance:
- How are these transports selected and utilized within the class?
- Do they introduce any performance or compatibility issues?
- How do these changes impact existing functionality and client behavior?
---------------------------------------------------------------------------------------
### Semantic Conflict Assessment:
Looking at the changes introduced in versions A and B, there does not seem to be a direct semantic conflict between them. Version A primarily deals with transport configuration and introduces a new field `transports` along with related methods. On the other hand, version B modifies the hostname verifier behavior.

However, when merging both changes, we need to ensure that the introduced hostname verifier (`OkHostnameVerifier`) is consistent with the overall configuration of the `OkHttpClient`. Specifically, we need to verify whether `OkHostnameVerifier` is compatible with the behavior and expectations of the existing codebase.
\end{response}

In a nutshell, the situations in which it identifies the conflicts are simple fabricated scenarios with significant alterations to behaviour. When the emergent behaviour is small and expected, such as those of PF, the LLM does not view it as a conflict, indicating a bias to clearly unexpected and unintended behaviour. On the other side, if the class is of significant complexity, it becomes harder for the LLM to extract valuable information.
We see these issues combined in OkHttp as the conflict is straighforward: parallel changes result in an object that is different from both branches, thus emergent. Simultaneously, the class is significantly more complex than the other subjects. Thus while in the PF examples, the possibility of a conflict is flat out dismissed,
two of the answers for OkHttp accept the possibility, arguing that more study needs to be done on transports and the hostname verifier respectively.

To improve identification, further work should on one hand explore how to provide a better understanding of semantic conflicts, specifically that emergent behaviour is the result of semantic conflict, even if it does not introduce an apparent bug, software fault or unexpected behaviour.
On the other hand, solutions need to be conceived for real-world scenarios with complex dependencies, large classes with varied functions, inheritance and other features that complicate detection. More complex prompts, data extraction and presentation are likely required.

\subsection{Conflict Explanation}

While the majority of cases recognized the existence or the possible existence of a conflict, the LLM could not always express what the conflict was and how it manifested.
In some cases, descriptions were entirely factually incorrect, such as ``If the user is an admin and a discount code that results in a cost of 0 is applied, the exception will never be thrown because the `checkout` method will not be called.'' for Cart (CM). Another case of this happened with the only OkHttp where a conflict was recognized:
\begin{response}
### Explanation:
The conflict arises because both changes affect the same field (`hostnameVerifier`), but they prescribe different default values.

In the merge commit, the decision was made to adopt the change from Version B, which explicitly sets the default `hostnameVerifier` to `OkHostnameVerifier()`. This decision potentially overrides any behavior or configuration related to the default `hostnameVerifier` set in Version A.

If the intended behavior was to preserve the default `hostnameVerifier` as defined in Version A, then this would be considered a semantic conflict, as the behavior of the merged version may differ from the behavior of either Version A or Version B individually.
\end{response}
In this case, the changes made in only one branch are described as a conflict.

However, most cases were of imprecisions and omissions, not entirely wrong descriptions. In some cases, these were the result of limitations of our prompt.
Particularly, with the ``Remove Override'' and ``Overload by Access Change'' subjects, the LLM was limited because the prompt only included information for the target class.
Thus, ChatGPT understand there would be a problem with Simultaneously adding a dependency to a method and removing its override, but lacking access to the method being overriden,
could not give an actual description of what the error would entail.
Better results were observed when doing more manual, ad-hoc experiments, as we could identify exactly all the classes involved in the conflict and offer all the required information.

Another aspect worth considering is that while in many cases, ChatGPT could reliably describe which elements and interactions were the origin of the conflict, it would or could rarely then explain how that conflict manifested, ie
what outputs would be returned. We observe this in the significance differences between metric D and E.

A notable exception is Cart (CM), with a 2.7 average for metric D. Indeed, we find it correctly explains everything, including how the code will behave.
\begin{response}
Now, in the merged version, both changes are included. So, when an admin user proceeds to checkout (assuming the admin check in version A evaluates to true), the cost will be set to 0. But immediately after, the checkout method checks if the cost is 0 and throws an exception if it is. This leads to a contradictory behavior: the system sets the cost to 0 for admin users but then doesn't allow them to proceed with checkout due to the cost being 0.

Therefore, the semantic conflict arises from the contradictory behaviors introduced by the changes in versions A and B, specifically regarding how the cost is handled for admin users and the checkout process.
\end{response}
It is possible this is due to the very straighforward logic of the CM example in particular: it is hard to describe the conflict without mentioning the relation between admin and the method throwing.
What is surprising is that due to some minor changes in the Cart class, we re-ran these tests and originally this metric was much lower. Why there was such an improvement is unknown but it is evidence of the variability of Large Language Models.

It is difficult to assess to what extent the type of conflict affects the correctness of explanations. If we observe PM and CM, Cart (PM) and Point (CM) are comparable. Cart (CM) has near perfect scores while OkHttp (PM) and Retrofit (PM/CM) fail completely.
It seems by far that the complexity of the conflict is the determining factor and this complexity can be noted as the complexity of the whole class under test (with OkHttp and Retrofit being the clear outliers), but also in the intricacies of the conflict.
We observe Cart (CM) is fairly straighforward: if the user is an admin, checkout throws and exception. Indeed, even without any of the merge info, it would be fairly trivial to identify this is unintended, whereas
Point and Cart (PM) require the identification of the original behaviour and how it has changed.

We can conclude that identification does not necessarily lead to a proper explanation. 
This is not a the most pressing issue, as a tool that identifies conflicts and then alerts humans, who can themselves describe and fix the issue would already be a great boon for efficient software development.
However, it limits our ability to further work on fully automated solutions, as explanations of the conflict are likely invaluable for this.


\section{RQ2: Given an explanation of a semantic conflict in a merge
commit, can ChatGPT generate unit test case that are able to identify it?}\label{sec:results:rq2}

\Cref{tab:results:rq2} summarizes the results achieved, organized by subject and
type of conflict.
Each column lists the passing tests, after edits, the average edit distance to tests that compile and pass in merge,
and the errors that stop the conflict from being detected.
%
\Cref{tab:results:rq2tests} reports the test results of the corrected tests.
These come in the format b (for Base), A, B, and M (for Merge). C stands for compilation failures, R for runtime errors, F for failing test, and P for passing tests.
From \Cref{tab:results:rq2tests} we can observe that:
\begin{itemize}
  \item The main obstacle to the detection of the conflict was incorrect asserts, present in nearly every case.
  \item In non-compiling tests, those generated with explanations given by ChatGPT tend to require more correction.
  \item Tests are generally compilable or very close to succesful compilation, with constructor and other errors, such as usage of non-existent getters as the main issues.
\end{itemize}

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} lrrr} \toprule
                             & Successes (out of 3) & Avg. Edit Distance & Errors \\
    \midrule
    Point (CM) Manual        & 3 & 4 & Assert \\
    Point (CM) ChatGPT       & 3 & 4 & Assert \\
    Fabricated (OAC) Manual  & 3 & 2 & Assert/Branch/Other \\
    Fabricated (OAC) ChatGPT & 3 & 4 & Assert/Branch \\
    Fabricated (RO) Manual   & 1 & 3 & Assert/Miss \\
    Fabricated (RO) ChatGPT  & 1 & 3 & Assert/Miss \\
    Cart (CM) Manual         & 3 & 48 & Assert/Constructor \\
    Cart (CM) ChatGPT        & 3 & 33 & Constructor/Other \\
    Cart (PF) Manual         & 3 & 2 & Assert \\
    Cart (PM) Manual         & 0 & 18 & Other \\
    Cart (PM) ChatGPT        & 1 & 42 & Assert/Other \\
    Antlr (PF) Manual        & 3 & 1 & Assert \\
    Retrofit (PM/CM) Manual  & 0 & N/A & Other \\
    \midrule
    \end{tabular}
    \caption{Results by conflict and explanation type, including succesful conflict identification (after edits), the average edit distance to a compiling and passing test as well as the errors found.
    OkHttp is omitted, as the LLM failed to generate any tests, only imports.
    \label{tab:results:rq2}}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} llcccccccccccccc}
        \toprule
        Prompt & & \multicolumn{14}{c}{Trial} \\
        \cmidrule(lr){3-16}
        & & \multicolumn{4}{c}{1} & \multicolumn{6}{c}{2} & \multicolumn{4}{c}{3} \\
                            & & b & A & B & M & & b & A & B & M & & b & A & B & M \\
\midrule
Point (CM) Manual           & & F & F & F & P & & F & F & F & P & & F & F & F & P \\
Point (CM) ChatGPT          & & F & F & F & P & & F & F & F & P & & F & F & F & P \\
                            & & F & F & P & P & & F & F & P & P & & F & F & P & P \\
Fabricated (OAC) Manual     & & C & C & F & P & & C & C & F & P & & C & C & F & P \\
Fabricated (OAC) ChatGPT    & & C & C & F & P & & C & C & F & P & & C & C & F & P \\
Fabricated (RO) Manual      & & C & F & C & P & & C & P & C & P & & C & P & C & P \\
Fabricated (RO) ChatGPT     & & C & F & C & P & & C & P & C & P & & C & P & C & P \\
Cart (CM) Manual            & & F & F & F & P & & F & F & F & P & & F & F & F & P \\
                            & &   &   &   &   & & P & P & P & P & &   &   &   &   \\
Cart (CM) ChatGPT           & & \textit{F} & \textit{F} & \textit{F} & \textit{P} & & \textit{F} & \textit{F} & \textit{F} & \textit{P} & & F & F & F & P \\ %italicize first two
                            & &   &   &   &   & &   &   &   &   & & F & F & P & P \\
Cart (PF) Manual            & & F & F & F & P & & F & F & F & P & & F & F & F & P \\
                            & & F & F & F & P & & F & F & F & P & & F & F & F & P \\
Cart (PM) Manual            & & C & C & P & P & & C & C & P & P & & C & C & P & P \\
                            & & P & P & P & P & & C & C & P & P & & P & P & P & P \\
                            & &   &   &   &   & & C & C & P & P & & C & C & P & P \\
                            & &   &   &   &   & & C & C & P & P & & P & P & P & P \\
Cart (PM) ChatGPT           & & C & C & P & P & & C & C & P & P & & C & C & P & P \\
                            & & C & C & P & P & & C & C & F & P & & P & P & P & P \\
                            & &   &   &   &   & & C & C & P & P & & C & C & P & P \\
                            & &   &   &   &   & & C & C & F & P & & P & P & P & P \\
Antlr (PF) Manual           & & F & F & F & P & & F & F & F & P & & F & F & F & P \\
        \bottomrule
    \end{tabular}
    \caption{Test compilation and running results in base (b), branch A,
    branch B, and merge (M).  C stands for compilation failures, R for runtime errors,
    F for failing test, and P for passing tests. The cases in italic indicates tests that were generated with deprecated Junit features.
    Most rows are found where multiple unit tests were generated.
    \label{tab:results:rq2tests}}
\end{table}


Analysis of \Cref{tab:results:rq2tests} allows us to identify which cases identify the conflict, and which did not.
It is worth examining the results and what they mean, starting by the failing examples:
%
\begin{itemize}
  \item PPPP: The test always passes, due to the correct testing of a feature not affected by the merge.

  \item FFPP and CCPP: The test passes in branch B and in merge. This indicates a behaviour was introduced and remained unaltered in the merge, thus it is not a conflict in itself.
  This was common for Point (CM) ChatGPT, as the prompt induced the generation of tests for \texttt{distance}, which correctly calculated and asserted the euclidean distance of a given point.  The test for the \texttt{move} method failed to the same, thus those required correction.

  \item CPCP: The same as the previous example, but the change introduced is in branch A.
\end{itemize}
%
The examples that show a semantic conflict is present require failure in all brances except merge, or compilation failure in base and one of the branches and failure on the other.
These are:
\begin{itemize}
  \item FFFP
  \item CCFP
  \item CFCP
\end{itemize}
%
All these examples show that behaviour is emergent: something which was never true in any of the previous branches becomes true. The behaviour being asserted is thus the
emergent behaviour that occurs in merge.

If we observe the results, particularly the errors found, we can see that the main issue present
is incorrect assertions. While in some cases these are understandable (such as the case of RO,
where the values to verify are hash and thus hard to know without running the code), most of the time
they were the result of simple logical or mathematical errors, things that are trivial for a human to get right
and thus also correct.
It is hard to identify why exactly it finds it so hard to get the right asserts. If we observe the cart example,
we find that tests generated for \texttt{distance} were always correct. These consisted of a simple \texttt{x + y} addition.
However, tests generated for \texttt{move} always had incorrect assertion values; in turn these were just \texttt{x + distance} and \texttt{y + distance} additions.
Given that it shows it can correctly calculate \texttt{distance} it is hard to fathom why it struggles
to use it in addition: we can perhaps suggest there is making the association between the method call and the returned value it represents.

Ocasional compilation errors occurred due to the generation of methods that are not present in the code, such as \texttt{getUser}.
In one case, we find an interesting error: the codebase contains methods named ``setResgistration''.
For the most part they are ignored, as they are not necessary to identify the conflict. In one trial, however,
the method is called for Producer. However the `Resgistration'' typo is corrected: ``producer.setRegistrationNumber(123);''.
Naturally, this stops code from compiling, but it is a noteworthy example of how LLMs, due to their nature as
predictive models, can fail in outlier cases, such as an unexpected typo in method namings.

It is noticeable that for Cart (CM), the manual explanation test cases required slight more editing, while the reverse
is observed for Cart (PM). An explanation might be that one type of prompt generates more complex and longer tests. However as can be seen in
\cref{tab:results:rq2tests}, both examples have comparable amount of test cases and test length does not vary much.
It is possible these are the results of outliers: that if we for example observe Cart (PM), ChatGPT explanation, the results are significantly weighed by
trial 3, which requires significant correction due to the way it tries to access the user:
\begin{response}
@Test
public void testSumItemsWithCoupon() {
    Cart cart = new Cart(new User());
    Item item = new Item(50, true); // Item with coupon
    cart.add_item(item);
    assertEquals(-50, cart.sum_items(), 0.001);
}

@Test
public void testSumItemsWithoutCoupon() {
    Cart cart = new Cart(new User());
    Item item = new Item(50); // Item without coupon
    cart.add_item(item);
    assertEquals(50, cart.sum_items(), 0.001);
}

@Test
public void testTotalCostWithCoupon() {
    Cart cart = new Cart(new User());
    Item item = new Item(50, true); // Item with coupon
    cart.add_item(item);
    cart.checkout("loyal");
    assertEquals(-45, cart.getUser().getBalance(), 0.001);
}

@Test
public void testTotalCostWithoutCoupon() {
    Cart cart = new Cart(new User());
    Item item = new Item(50); // Item without coupon
    cart.add_item(item);
    cart.checkout("loyal");
    assertEquals(-45, cart.getUser().getBalance(), 0.001);
}
\end{response}
has an edit distance of 121, has correction requires extracting the user for access:
\begin{response}
@Test
public void testSumItemsWithCoupon() {
    Cart cart = new Cart(new User());
    Item item = new Item(50, true); // Item with coupon
    cart.add_item(item);
    assertEquals(0, cart.sum_items(), 0.001);
}

@Test
public void testSumItemsWithoutCoupon() {
    Cart cart = new Cart(new User());
    Item item = new Item(50); // Item without coupon
    cart.add_item(item);
    assertEquals(0, cart.sum_items(), 0.001);
}

@Test
public void testTotalCostWithCoupon() {
    User user = new User();
    Cart cart = new Cart(user);
    Item item = new Item(50, true); // Item with coupon
    cart.add_item(item);
    cart.checkout("loyal");
    assertEquals(0, user.balance, 0.001);
}

@Test
public void testTotalCostWithoutCoupon() {
    User user = new User();
    Cart cart = new Cart(user);
    Item item = new Item(50); // Item without coupon
    cart.add_item(item);
    cart.checkout("loyal");
    assertEquals(0, user.balance, 0.001);
}
\end{response}
This same error of trying to access user from cart can be found in other ChatGPT results in PM, but it is ambiguous
whether this is somehow the result of the prompt or coincidental generations. On the other side, Cart (CM) tended to generate
asserts of type AssertThrow and AssertNotThrow, which often had to be reversed to be in line with merge, for manual, whereas for
ChatGPT explanation the conflict these asserts were more often correct. This is clearly an issue with the manually written explanation,
which claims the error throwing behaviour in merge is not intended, leading for tests that assert that nothing is thrown.
It is thus likely good practice to not make value judgements on the behaviour of the code in merge.

On the topic of compilation errors, it is worth nothing often, for Cart (CM) these were related to constructors, as the constructors
for the other classes involved in the Cart conflicts (Item and User) were not included in the prompt. It is interesting
to observe what the common error is: the introduction of extra parameters. For example, in the code of Cart only the fields
value and amount of Item are accessed. However, Item was frequently iniatilized with a constructor with three parameters: Name,
Value and Amount. It is fair to assume the Item class would both have a name field and that it would come first in the constructor
and it is likely the training data had many examples of this. Thus it is worth pointing out that testability of code by LLMs can be
improved by closely following norms and rule of thumb expectations and corollarily LLMs will struggle to effectively construct tests
for more unusual pieces of software.

A way to resolve this issue would be to include constructor headers in the original prompt. However one issue is the selection of which
headers: if instead of just Item and User, the Cart class had 20 other dependencies which bore no relevance in our conflict, we would be
providing extra information in the prompt which could cause confusion. A possibly better option is feeding compilation messages back for correction,
as shown by ChatTester~\cite{kn:chattester}, possibly with the addition of corresponding headers. This corrective approach could also be utilized
to fix assert errors. In effect, it would be an automated way to do the manual corrections we carried out.

Some other issues with constructors we observed are not so obvious to solve: namely that despite knowing of available constructors, there is no guarantee
the correct one for the situation will be used. We can see the case of Cart (PM): due to changes in the Item class, constructor information came included
in the diff section of the prompt. This led to the creation of tests using (Value, Coupon) and (Value) format.
\begin{response}
    @Test
    public void testSumItemsWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        assertEquals(-50, cart.sum_items(), 0.001);
    }

    @Test
    public void testSumItemsWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        assertEquals(50, cart.sum_items(), 0.001);
    }

    @Test
    public void testTotalCostWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }
\end{response}
Notably these constructors don not set the amount field, which is thus 0, rendering the items useless for any value calculations.
Rectification would require understanding this logic error and thus is liable to be more difficult to achieve. It is for these representations
we have so few successes for Cart (PM): while we corrected the asserts to get passing tests in merge (in this case, setting them to 0, as no
value is subtracted), these corrections do not aid in identifying the conflict.

The other case where the conflict is rarely identified despite our corrections is RO, as it often generated asserts which always passed:
\begin{response}
@Test
public void testHashString() {
    InstalacaoAssentos instalacao = new InstalacaoAssentos();
    assertNotNull(instalacao.hashString());
}
\end{response}
Within the parameters of our work and a corrective system that works off compilation and test errors, this is the most problematic case: as the test works perfectly in merge,
there is nothing prompting us for repair and repair would require more than simply changing a value, but to actually change the type of assert itself.

Retrofit stands as an outlier and indeed we did correct the generated tests as they are so deviated from correctness, any edit would have to fundamentally alter the setup and structure of the test,
to a point where if henceforth the test worked, we could not say it reflected anything on ChatGPT's capabilities.
From observing, it is clear the LLM in a way does not have enough information to correctly generate tests and thus opts for generating non-existent mocks:
\begin{response}
@Test(expected = IllegalArgumentException.class)
public void testBuildRelativeUrlWithNullPathParam() throws UnsupportedEncodingException {
    RequestBuilder builder = new RequestBuilder(mockConverter());
    builder.methodInfo(mockMethodInfo());
    builder.apiUrl("http://example.com");
    builder.args(new Object[]{null}); // Pass null argument
    builder.build(); // This should throw IllegalArgumentException
}
\end{response}
As well as extensive incomplete test suites, which end up being almost all the same.
\begin{response}
@Test(expected = IllegalArgumentException.class)
public void testBuildRelativeUrl_NullPathParam() throws UnsupportedEncodingException {
    RequestBuilder builder = new RequestBuilder(mock(Converter.class));
    builder.methodInfo(mock(RestMethodInfo.class))
           .apiUrl("http://example.com")
           .args(new Object[] {null})
           .build();
}

@Test(expected = IllegalArgumentException.class)
public void testBuildBody_NullBody() {
    RequestBuilder builder = new RequestBuilder(mock(Converter.class));
    builder.methodInfo(mock(RestMethodInfo.class))
           .apiUrl("http://example.com")
           .args(new Object[] {null})
           .build();
}

@Test(expected = IllegalArgumentException.class)
public void testBuildBody_NullFormUrlEncodedField() {
    RequestBuilder builder = new RequestBuilder(mock(Converter.class));
    builder.methodInfo(mock(RestMethodInfo.class))
           .apiUrl("http://example.com")
           .args(new Object[] {null})
           .build();
}

@Test(expected = IllegalArgumentException.class)
public void testBuildBody_NullMultipartPart() {
    RequestBuilder builder = new RequestBuilder(mock(Converter.class));
    builder.methodInfo(mock(RestMethodInfo.class))
           .apiUrl("http://example.com")
           .args(new Object[] {null})
           .build();
}
\end{response}
It is worth noting none of the tests seem to attempt to test both modified behaviours at once (the addition of the character '?' to query parameters and null handlings). Thus even if correctly designed, they would not detect the conflict.
However this could always be a prompt issue.

It is worth nothing Retrofit has a large existing test suite, including a Helper class which is very helpful for setting up the RequestBuilder with appropriate arguments.
Exploratory work had shown that feeding this test class and prompting for generation on its style could lead to the generation of working (but not detecting) tests. It is probable then,
that for these complex real-world scenarions, aiding test generation by giving examples of existing test suites will be a necessary part of the prompt. Indeed, even if ChatGPT could generate
working tests regardless, it would still be useful for developers to have them more closely conform the existing suite.

Despite all the problems mentioned previously, the results are generally quite good, as most errors are relatively small and easily fixable with further prompting or manual edits as we did.
Thus we can say with some certainty that for these simple cases, most of which where ChatGPT can identify the conflict with some degree of confidence, it can also generate appropriate unit tests.
It is unfortunate that test generation for OkHttp failed so badly was to not even generate tests, as that class was far more intricate and could give evidence to issues such as dealing with
private methods. But it has to be understood that these sort of issues are a possibility given the volatility of LLMs.



\section{RQ3: Can state-of-the-art prompts lead ChatGPT to generate to
unit test cases that are able to identify semantic conflicts?  How do their
results compare to the results of the prompt used in RQ2?}\label{sec:results:rq3}


%\begin{table}[t]
%    \centering
%    \begin{tabular}{@{\extracolsep{\fill}} lrr} \toprule
%                     & Test Results & Possible Identification \\
%    \midrule
%    Point (CM)       & Passing & No \\
%    Fabricated (RO)  & Failing & Yes \\
%    Fabricated (OAC) & Non-Compiling & Yes \\
%    Cart (CM)        & Non-Compiling & No \\
%    Cart (PF)        & Passing & Yes \\
%    Cart (PM)        & Non-Compiling & Yes \\
%    Antlr (PF)       & Non-Compiling & No \\
%    OkHttp (PM)      & Non-Compiling & No \\
%    Retrofit (PM/CM) & Non-Compiling & No \\
%    %\midrule
%    \bottomrule
%    \end{tabular}
%    \caption{Results for Prompt 1.\label{tab:results:rq3p1}}
%\end{table}

%\begin{table}[t]
%    \centering
%    \begin{tabular}{@{\extracolsep{\fill}} lrr} \toprule
%                     & Test Results & Possible Identification \\
%    \midrule
%    Point (CM)       & Failing & Yes \\
%    Fabricated (RO)  & Passing & No \\
%    Fabricated (OAC) & Failing & Yes \\
%    Cart (CM)        & Non-Compiling & Yes \\
%    Cart (PF)        & Non-Compiling & No \\
%    Cart (PM)        & Non-Compiling & Yes \\
%    Antlr (PF)       & Non-Compiling & No \\
%    OkHttp (PM)      & Non-Compiling & No \\
%    Retrofit (PM/CM) & Non-Compiling & No \\
%    %\midrule
%    \bottomrule
%    \end{tabular}
%    \caption{Results for Prompt 2.\label{tab:results:rq3p2}}
%    \end{table}

%\begin{table}[t]
%    \centering
%    \begin{tabular}{@{\extracolsep{\fill}} lrr} \toprule
%                     & Test Results & Possible Identification \\
%    \midrule
%    Point (CM)       & Runtime & No \\
%    Fabricated (RO)  & Failing & No \\
%    Fabricated (OAC) & Non-Compiling & No \\
%    Cart (CM)        & Non-Compiling & No \\
%    Cart (PF)        & Passing & Yes \\
%    Cart (PM)        & Non-Compiling & Yes \\
%    Antlr (PF)       & Non-Compiling & Yes \\
%    OkHttp (PM)      & Non-Compiling & \todo{hostnameVerifier} \\
%    Retrofit (PM/CM) & Non-Compiling & No \\
%    %\midrule
%    \bottomrule
%    \end{tabular}
%    \caption{Results for Prompt 3.\label{tab:results:rq3p3}}
%    \end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} l*{8}{c}} \toprule
                      & \multicolumn{2}{c}{Prompt 1} & \multicolumn{2}{c}{Prompt 2} & \multicolumn{2}{c}{Prompt 3} & \multicolumn{2}{c}{Prompt 4}\\
                      & Comp. & Pass. & Comp. & Pass. & Comp. & Pass. & Comp. & Pass. \\
    \midrule
    Point (CM)       & 3 & 2 & 1 & 0 & 0 & 0 & 1 & 1 \\
    Fabricated (RO)  & 1 & 0 & 1 & 1 & 3 & 0 & 1 & 0 \\
    Fabricated (OAC) & 2 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    Cart (CM)        & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    Cart (PF)        & 2 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
    Cart (PM)        & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
    Antlr (PF)       & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    OkHttp (PM)      & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
    Retrofit (PM/CM) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \midrule
    Trials    & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{1} \\
    \bottomrule
    \end{tabular}
    \caption{Results for Prompt 1, 2, 3 and 4, listing how many tests compile (or run without errors) and from those, how many tests pass.\label{tab:results:rq3t1}}
\end{table}

%compilation/runtime

\Cref{tab:results:rq3t1} allows us to observe that issues relating to compilation and correct assertions remain
a problem for these prompts, confirming this is not necessarily just a issue with the prompt we developed.
%
Particular prominent issues like the issue of incorrect constructors %, such as in the case of Item,
remained relevant
and more prevalent as more test cases were generated for each prompt.

Moreover, new issues with the usage of non-existent methods arose. Whereas before these largely happened with the classes that were not present
in the prompt, we observed the adaptation of the \texttt{add\_item(Item)} method into one that took an list input, in Prompt 3:
\begin{response}
    @Before
    public void setup() {
        user = new User("John Doe");
        cart = new Cart(user);
        cartItems = Arrays.asList(new Item("Item1", 10.0, 2, false),
                                  new Item("Item2", 20.0, 1, true));
        cart.add_items(cartItems);
    }
\end{response}
% It is possible
The reason for this is that the prompt only shows the method signature, which makes it less impactful and thus the LLM may be
more prone to adapt or use it incorrectly. In this case, it is likely a method to add items in this manner was very common on the training dataset,
which goes in line with our previous assessments that issues often arise when our implementation deviates from what is common or expected.

Of note is the case of Prompt 3: while it prompted the LLM to use mocks and reflection when possible, it used these features
for all cases, even though these were unnecessary for the vast majority of cases. It did however generate one passing test for OkHttp,
which so far we had failed to do and where other prompts failed too, as they all sought to access the private method copyWithDefaults().
Of note, however, the two compilation failures for OkHttp in Prompt 3 also dealt with private access, but to a field rather than a method.

Prompt 4 suffered from limitations of generation and it would often stop generating midtest. For our purposes these were cropped and not counted for the analysis.
This limitation makes sense, as Copilot is primarily targeted at helping developers complete code, as an auxiliary tool, not as a tool to generate very large
code blocks without developer interference.

While we have several passing tests, few are useful in accurately identifying the conflict.
These were Cart (PF) for Prompt 1 as well as Cart (PM) for Prompt 3.


\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} lrrrr} \toprule
                     & Prompt 1 & Prompt 2 & Prompt 3 & Prompt 4 \\
    \midrule
    Point (CM)       & 1 & 1 & 0 & 1 \\
    Fabricated (RO)  & 1 & 0 & 0 & 1 \\
    Fabricated (OAC) & 3 & 1 & 0 & 1 \\
    Cart (CM)        & 0 & 1 & 0 & 0 \\
    Cart (PF)        & 3 & 1 & 1 & 1 \\
    Cart (PM)        & 2 & 0 & 2 & 1 \\
    Antlr (PF)       & 1 & 1 & 0 & 1 \\
    OkHttp (PM)      & 0 & 0 & 0 & 0 \\
    Retrofit (PM/CM) & 0 & 0 & 0 & 0 \\
    Trials           & 3 & 1 & 3 & 1 \\
    %\midrule
    \bottomrule
    \end{tabular}
    \caption{Number of tests that identify the conflict, if simple corrections are made such as in RQ2\label{tab:results:rq3t2}}
\end{table}

In \Cref{tab:results:rq3t2}, we can observe that by applying the same correction process we carried out in
RQ2, we can achieve a meaningful number of tests that not only pass, but also identify the semantic conflict.

It is however notable that these are fewer in number, except maybe Prompt 4, which can be corrected for six subjects.
While we find failures for the same reason as in RQ2, where after correction tests pass, but do not identify the conflict,
the characteristics of these prompts also introduced new issues.

One of these relates to Change Method type conflicts: given Prompt 1 and 3 only carry method signatures for non-focal methods,
the relationship between the affected methods is hidden. Thus, for example, no test is generated for Cart (CM) that investigates
checkout with an admin type user as there is not even information that users can have that characteristic. In other cases, it either
calls or mocks methods to ascertain the appropriate assert. This is the case with Point, for example for Prompt 1:
\begin{response}
    @Test
    public void testMove() {
        Point point = new Point(3, 4); // Create a Point object at coordinates (3,4)

        double initialDistance = point.distance(); // Calculate initial distance from origin
        point.move(); // Move the point

        // Assert that the new coordinates are equal to the initial coordinates plus the initial distance
        assertEquals(3 + initialDistance, point.getX(), 0.0001);
        assertEquals(4 + initialDistance, point.getY(), 0.0001);
    }
\end{response}
As well as for Prompt 3:
\begin{response}
    @Test
    public void testMove() throws Exception {
        // Mocking distance method
        when(point.distance()).thenReturn(5.0);

        // Using reflection to invoke private method move()
        Point.class.getDeclaredMethod("move").invoke(point);

        // Verifying the new values of x and y after calling move()
        assertEquals(5.0, point.getX(), 0.0);
        assertEquals(5.0, point.getY(), 0.0);
    }
\end{response}
The former does not identify the conflict because the assert, rather than being fixed on the merge behaviour,
varies depending on the result of distance. The latter does not identify it because the behaviour of distance is
obscured.

Prompt 3 specifically suffer from this degree of excessive mocking when not necessary, either obscuring the behaviour of
the actual code under test or generating tests that cannot be corrected easily, without reworking the test in general.

The most significant difference, if we dismiss the specific case of Prompt 3, is that while we can achieve succesful results,
there is much higher variability: in most cases for RQ2, if one trial could be ammended, all three could be, while here we observe much
higher variability. For example, Prompt 1, for Antlr, generates:
\begin{response}
    @Test
    public void testGetBadWords() {
        Python2Target python2Target = new Python2Target(new CodeGenerator());
        Set<String> badWords = python2Target.getBadWords();

        assertNotNull(badWords);
        assertFalse(badWords.isEmpty());

        // Check some known bad words
        assertTrue(badWords.contains("print"));
        assertTrue(badWords.contains("execfile"));
        assertTrue(badWords.contains("lambda"));

        // Since the set is populated, call getBadWords() again should return the same set
        Set<String> badWordsSecondCall = python2Target.getBadWords();
        assertEquals(badWords, badWordsSecondCall);
    }
\end{response}
This is a perfectly valid test and it verifies several importants features of the getBadWords method: it should
return a non-null, non-empty set, that contains some words we know and should be constant. However to identify the semantic
conflict we need to check the size.

A way to offset this higher variability is simply to generate more tests. Indeed, Prompt 2 specifically indicates the number of
tests to generate, which we specified at 10 as was done before~\cite{kn:siddiq2023empirical}. While it did not always generate the right number, it generated many
tests and often the correct one for our purposes came later. For Cart (CM), the test we want is the 4th, testCheckoutWithAdminUser:
\begin{response}
    class CartTest {

    private Cart cart;
    private User user;

    @BeforeEach
    void setUp() {
        user = new User("John", false, 100.0);
        cart = new Cart(user);
    }

    @Test
    void testCheckoutWithValidCode() {
        Item item = new Item("Item1", 10.0, 2);
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(80.0, user.balance);
    }

    @Test
    void testCheckoutWithInvalidCode() {
        Item item = new Item("Item2", 20.0, 1);
        cart.add_item(item);
        assertThrows(NullPointerException.class, () -> cart.checkout("invalid"));
    }

    @Test
    void testCheckoutWithZeroCost() {
        Item item = new Item("Item3", 30.0, 1);
        cart.add_item(item);
        assertThrows(RuntimeException.class, () -> cart.checkout("30ff"));
    }

    @Test
    void testCheckoutWithAdminUser() {
        user.setAdmin(true);
        Item item = new Item("Item4", 40.0, 1);
        cart.add_item(item);
        cart.checkout("superpromo");
        assertEquals(100.0, user.balance);
    }

    @Test
    void testCheckoutWithMultipleItems() {
        Item item1 = new Item("Item5", 10.0, 2);
        Item item2 = new Item("Item6", 20.0, 1);
        cart.add_item(item1);
        cart.add_item(item2);
        cart.checkout("2024");
        assertEquals(50.0, user.balance);
    }

    @Test
    void testCheckoutWithEmptyCart() {
        assertThrows(RuntimeException.class, () -> cart.checkout("superpromo"));
    }

    @Test
    void testSumItems() {
        Item item1 = new Item("Item7", 10.0, 2);
        Item item2 = new Item("Item8", 20.0, 1);
        cart.add_item(item1);
        cart.add_item(item2);
        assertEquals(40.0, cart.sum_items());
    }

    @Test
    void testTotalCost() {
        Item item = new Item("Item9", 10.0, 2);
        cart.add_item(item);
        assertEquals(18.0, cart.total_cost(0.1));
    }

    @Test
    void testTotalCostWithAdminUser() {
        user.setAdmin(true);
        Item item = new Item("Item10", 20.0, 1);
        cart.add_item(item);
        assertEquals(0.0, cart.total_cost(0.25));
    }
}
\end{response}

By increasing the number of tests generated we increase the odds of generating the test we are looking for,
but these come at higher costs: more tokens need to be generated, more time is spent generating and a lot more
correction is required. The monetary and time costs are thus much higher.

Prompt 4 also indicated the generation of 10 tests, but Copilot showed to be limited in how much it would generate at once,
leading to less tests per generation. Furthermore tests showed less variance, with variation being mostly on values, not the
test logic. For example, for the same subject:
\begin{response}
    @Test
    void testCheckout() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(990, user.balance);
    }

    @Test
    void testCheckout2() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("superpromo");
        assertEquals(985, user.balance);
    }

    @Test
    void testCheckout3() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("30ff");
        assertEquals(980, user.balance);
    }

    @Test
    void testCheckout4() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("2024");
        assertEquals(990, user.balance);
    }

    @Test
    void testCheckout5() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("invalid");
        assertEquals(1000, user.balance);
    }

    @Test
    void testCheckout6() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(990, user.balance);
    }

    @Test
    void testCheckout7() {
        User user = new User("John", 1000, false);
        Cart cart = new Cart(user);
        Item item = new Item("item1", 10, 1);
        cart.add_item(item);
    }
\end{response}
It is worth nothing for this comparison we closely followed Prompt 2's format for 4, with the main variable being the usage of Copilot
instead of ChatGPT. While in this case the metrics between these two were comparable, we have to note that better results, particularly
in regards to compilation, might have been achieved if we had some adopted some known good practices for Copilot generation, such as giving
context of existing tests~\cite{kn:githubcopilot}. But it is also true that test context might also be applicable to our ChatGPT prompts,
after adaptation. The downside is this requires existing tests, which should usually be the case but not always.

In general we verify then that yes, while existing prompts can produce similar results to the prompt developed for RQ2
and can in some cases test the emergent behaviour, this comes with higher randomness and thus associated effort.
By prompting with the explanation of the conflict provided, we can achieve positive results more reliably as the
LLM is given the exact information required on what behaviour needs to be verified.
