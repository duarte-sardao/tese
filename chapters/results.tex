    \chapter{Experimental results}\label{chap:results}

This chapters answers \textbf{RQ1 â€“ RQ?}, which study \todo{???}.

\section{RQ1}\label{sec:results:rq1}

\begin{table}[t]
\centering
% \tabcolsep=1cm
% \renewcommand{\arraystretch}{0.90}
\begin{tabular}{@{\extracolsep{\fill}} lrrrrr} \toprule
                 & A [0-3] & B [0-1] & C [0-2] & D [0-3] & E [0-3] \\
\midrule
Point (CM)       & 3.0 & 1.0 & 0.7 & 1.0 & 0.7 \\
Fabricated (RO)  & 3.0 & 1.0 & 1.0 & 2.0 & 0.7 \\
Fabricated (OAC) & 2.3 & 1.0 & 1.0 & 3.0 & 0.7 \\
Cart (CM)        & 3.0 & 1.0 & 2.0 & 3.0 & 2.7 \\
Cart (PF)        & 1.0 & 1.0 & 2.0 & 3.0 & 1.7 \\
Cart (PM)        & 2.0 & 1.0 & 1.3 & 1.0 & 0.3 \\
Antlr (PF)       & 1.7 & 1.0 & 0.0 & 0.0 & 0.0 \\
OkHttp (PM)      & 3.0 & 1.0 & 0.7 & 0.0 & 0.0 \\
\midrule
\textit{Average}  & 2.4 & 1.0 & 1.1 & 1.6 & 0.8 \\
\bottomrule
\end{tabular}
\caption{Average values per subject and metric.\label{tab:results:rq1}}
\end{table}

\Cref{tab:results:rq1} summarizes the metrics achieved per subject. From these we can note that:

\begin{itemize}
  \item \textbf{ChatGPT is generally able to identify changes made between branches} ($A=2.4$).

  \item \textbf{In none of the subject did the tool mistake the kind of conflict under discussion} ($B=1.0$). This might be because the prompt did not assert the existence of a conflict, allowing negative responses, whereas other prompts, which asked to find a conflict that did exist, lead the model to find any explanation, which often consisted of describing a textual conflict.

  \item \textbf{Answers tend to identify a conflict}, but with uncertainty, e.g., with words such as ``possible'', and ``might'' ($C=1.1$). In other cases, the cases where no conflict was found outweighed the cases where one was found with uncertainty.

  \item \textbf{On average, it can identify what originates conflicts} ($D=1.6$).  Despite the range given, values given to the metric was either 0 or 3. This indicates to us in nearly half the cases, the model failed to produce any accurate description, but when it did, it was generally very accurate.

  \item \textbf{ChatGPT struggles with accurately describing the results of semantic conflicts} ($E=0.8$), i.e., how the behaviour is specifically altered and how the outputs change. Indeed, even when the origin of the conflict is found, an accurate description of the results does not necessarily follow, with the topic being vague or not touched upon.
\end{itemize}

The complete prompts, with results and evaluated metrics, can be found in the attachments \todo{Which one? Do not forget to cref it here}.
\todo{how to ref? can ref html files or to put in body of text?}
An important observation to make is the variability of results, which highlights the importance of running three trials for our evaluation.
For example, the point class had high variation: two of the results failed to identify any conflict at all, but one of the three not only identified, but quite accurately describe its origin and results.

In the more complex fabricated examples, the biggest issue was the identification of the effects of the semantic conflict, namely that when an overriden function was removed or had its access changed, the other function, with different behaviour, would be used instead. However, given the constraints of the prompt, which is limited to one class, the necessary information could not be transmitted and the LLM argued there would be compilation errors. 
Better results were observed when doing more manual, ad-hoc experiments, as we could identify exactly all the classes involved in the conflict and offer all the required information.

As we proceeded testing the ``Cart'' scenario, we observed a reversal of a trend of very low results for the E metric, particularly for CM and PM.
Indeed for these examples, not only did the LLM identify the origins of the conflict, but exactly the result of this, such as:
``Now, in the merged version, both changes are included. So, when an admin user proceeds to checkout (assuming the admin check in version A evaluates to true), the cost will be set to 0. But immediately after, the checkout method checks if the cost is 0 and throws an exception if it is. This leads to a contradictory behavior: the system sets the cost to 0 for admin users but then doesn't allow them to proceed with checkout due to the cost being 0.''
It is possible this is due to the very straighforward logic of the CM example in particular. /todo{include this next bit?} What is surprising is that due to some minor changes in the Cart class, we re-ran these tests and originally these values were much lower. Why there was such an improvement is unknown but it is evidence of the variability of these tools.

In a scenario where detection failed we see an example of the LLM providing a grossly wrong explanation: ``If the user is an admin and a discount code that results in a cost of 0 is applied, the exception will never be thrown because the `checkout` method will not be called.''. As regards the conflict itself, this was only completely wrong assertion of its kind: in general where answers failed they were either vague or omitted, not present. The only other major example of a factually incorrect answer came with OkHttp:
\begin{lstlisting}
### Explanation:
The conflict arises because both changes affect the same field (`hostnameVerifier`), but they prescribe different default values.
    
In the merge commit, the decision was made to adopt the change from Version B, which explicitly sets the default `hostnameVerifier` to `OkHostnameVerifier()`. This decision potentially overrides any behavior or configuration related to the default `hostnameVerifier` set in Version A.
    
If the intended behavior was to preserve the default `hostnameVerifier` as defined in Version A, then this would be considered a semantic conflict, as the behavior of the merged version may differ from the behavior of either Version A or Version B individually.
\end{lstlisting}
In this case, the changes made in only one branch are described as a conflict.

Another case of wrong information given by ChatGPT regarded the changes made, both in Cart(PF):
\begin{lstlisting}
1. Version A added a new discount code `"megasaver"` with a discount of 0.4d.
2. Version B changed the discount for the `"superpromo"` code from 0.25d to 0.4d.
------------------------------------------------------------------------
2. **Diff Version A**:
   - Adds a new discount code "megasaver" with a discount value of 0.4d to the `discounts` map.

3. **Diff Version B**:
   - Modifies the discount value for the "superpromo" code to 0.25d.
\end{lstlisting}
As well as Antlr:
\begin{lstlisting}
1. **Diff A vs Base:**
    - In version A, the `reduce` keyword was replaced with `return` in the `python2Keywords` array.
 
2. **Diff B vs Base:**
    - In version B, the `del` keyword was added to the `python2Keywords` array.
-------------------------------------------------------------------------------
1. **Diff A vs. Base**:
    - Addition of `"return"` keyword in the `python2Keywords` array.
    - No conflicting changes with other methods or properties.
 
2. **Diff B vs. Base**:
    - Replacement of `"delattr"` with `"del"` in the `python2Keywords` array.
    - No conflicting changes with other methods or properties.
\end{lstlisting}
However this did not worsen its ability to identify the conflict in the case of Cart, where the description of the conflict as in line with the changes made.
We observed both of these are ``Parallel Field'' semantic conflicts so it is possible the fact that the changes are being made to fields may have some relation to its inability to express them correctly. Further credence to this involves Cart (PM), where edits to the field of an auxiliary class also were misdescribed.

As the most complex example, OkHttp generated the least conclusive answers. In particular it tended to offer advice on how identify and diagnose semantic conflicts and make good merges. This is a behaviour that was also observed with other complex, real-world conflicts, during the period of ad-hoc testing.
\begin{lstlisting}
### Potential Semantic Conflict:
While there is no direct semantic conflict, there could be conflicts in the usage and interpretation of the `transports` field and its associated methods. For instance:
- How are these transports selected and utilized within the class?
- Do they introduce any performance or compatibility issues?
- How do these changes impact existing functionality and client behavior?
---------------------------------------------------------------------------------------
### Semantic Conflict Assessment:
Looking at the changes introduced in versions A and B, there does not seem to be a direct semantic conflict between them. Version A primarily deals with transport configuration and introduces a new field `transports` along with related methods. On the other hand, version B modifies the hostname verifier behavior.

However, when merging both changes, we need to ensure that the introduced hostname verifier (`OkHostnameVerifier`) is consistent with the overall configuration of the `OkHttpClient`. Specifically, we need to verify whether `OkHostnameVerifier` is compatible with the behavior and expectations of the existing codebase.
\end{lstlisting}

Evaluating the results as a whole, it is worth imagining in which way the types of conflicts affect the model's capabilities. As observed earlier, it seem that Parallel Field conflicts induce errors in branch change description. However, it does not follow that this results in a failure of detection and description, as Cart (PF) shows above average results (although Antlr, also PF, fails to detect any conflicts at all). 
It is difficult to assess to what extent the type of conflict affects detection ability. If we observe PM and CM, Cart (PM) and Point (CM) are comparable, while Cart (CM) has near perfect scores and OkHttp (PM) very low ones.
It seems by far that the complexity of the conflict is the determining factor and this complexity can be noted as the complexity of the whole class under test (with OkHttp being the clear outlier), but also in the intricacies of the conflict.
We observe Cart (CM) is fairly straighforward: if the user is an admin, checkout throws and exception. Indeed, even without any of the merge info, it would be fairly trivial to identify this is unintended, whereas
Point and Cart (PM) require the identification of the original behaviour and how it has changed.

It is also worth pointing out that despite good results for metric D in Remove Override and Overload by Access Change, with a perfect 3 in the latter, explanations of the result was flawed, due to a lack of information, ChatGPT reliably understood that trying a function that was removed or that had its access changed would produce errors. Thus it is worth noting in this case, accurate identification of the conflict's result would require a prompt that provides further information.

Overall, we can say ChatGPT can indeed identify and explain whether there is a semantic conflict in a merge. This comes with many caveats however, firstly being that identification does not necessarily lead to a proper explanation. This is not a particularly pressing issue, as a tool that identifies conflicts and then alerts humans, who can themselves describe and fix the issue would already be a great boon for efficient software development.

The bigger caveat is the situations in which it identifies the conflicts: simple fabricated scenarios, whose changes are simple to identify and immediately clearly related. In real world scenarios, where its not immediately obvious what functions the changes serve and how they relate to each other. To resolve this, it might necessary to work out how to efficiently feed the LLM so it can gain an accurate understanding of the whole system without losing focus on the specific task at hand.

\section{RQ2}\label{sec:results:rq2}

\Cref{tab:results:rq2} summarizes the results achieved, split by conflict and evaluation type.
These list the passing tests, without edits, the average edit distance to compilable tests and the errors present that stop the conflict from being detected.

\Cref{tab:results:rq2tests} gives the values for the tests, after compilation errors are fixed in merge.
These come in the format Base/AB/Merge. C stands for compilation failures, R for runtime errors, F for failing test and P for passing tests.
In some cases there are several listing per trial, due to the existence of several unit tests.

From this we can observe:
-The main obstacle to the detection of the conflict was incorrect asserts, present in nearly every case.
-In non-compiling tests, those generated with explanations given by ChatGPT require more correction.
-Tests are generally compilable or very close to succesful compilation, with constructor errors being the main issue.

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} lrrr} \toprule
                             & Successes (of 3) & Average Edit Distance & Errors \\
    \midrule
    Point (CM) Manual        & 0 & 0 & Assert \\
    Point (CM) ChatGPT       & 0 & 0 & Assert \\
    Fabricated (OAC) Manual  & 1 & 1 & Assert/Other \\
    Fabricated (OAC) ChatGPT & 2 & 0 & Assert \\
    Fabricated (RO) Manual   & 0 & 0 & Assert/Other \\
    Fabricated (RO) ChatGPT  & 0 & 0 & Assert/Other \\
    Cart (CM) Manual         & 0 & 9 & Assert/Constructor \\
    Cart (CM) ChatGPT        & 0 & 34 & Constructor/Other \\
    Cart (PF) Manual         & 0 & 6 & Assert/Constructor/Branch \\
    Cart (PF) ChatGPT        & 0 & 33 & Assert/Constructor/Branch \\
    Cart (PM) Manual         & 0 & 15 & Assert/Other \\
    Cart (PM) ChatGPT        & 1 & 37 & Assert/Other \\
    \midrule
    \end{tabular}
    \caption{Results by conflict and explanation type.\label{tab:results:rq2}}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{@{\extracolsep{\fill}} lrrr} \toprule
                             & Trial 1 & Trial 2 & Trial 3 \\
    \midrule
    Point (CM) Manual        & F/FF/F & F/FF/F & F/FF/F \\
    Point (CM) ChatGPT       & F/FF/F & F/FF/F & F/FF/F \\
                             & F/FP/P & F/FP/P & F/FP/P \\
    Fabricated (OAC) Manual  & C/CP/F & C/CP/F & C/CF/F \\
    Fabricated (OAC) ChatGPT & C/CF/F & C/CP/F & C/CP/F \\
    Fabricated (RO) Manual   & C/FC/F & C/PC/P & C/PC/P \\
    Fabricated (RO) ChatGPT  & C/FC/F & C/PC/P & C/PC/P \\
    Cart (CM) Manual         & P/PP/F & F/FF/F & P/PP/F \\
                             &  & F/FF/F &  \\
    Cart (CM) ChatGPT        & F/FF/P* & F/FF/P* & F/PF/F \\
                             &  &  & F/FP/P \\
    Cart (PF) Manual         & R/FF/F & R/PF/P & R/FF/F \\
    Cart (PF) ChatGPT        & R/FF/F & R/FF/F & R/FF/F \\
    Cart (PM) Manual         & C/CP/P & C/CP/P & C/CF/F \\
                             & P/PP/P & C/CF/F & F/FF/F \\
                             &  & C/CF/F & C/CF/F \\
                             &  & C/CF/F & F/FF/F \\
    Cart (PM) ChatGPT        & C/CF/F & C/CP/P & C/CF/F \\
                             & C/CF/F & C/CF/P & F/FF/F \\
                             &  & C/CP/P & C/CF/F \\
                             &  & C/CF/F & F/FF/F \\
    \midrule
    \end{tabular}
    \caption{Test running results in Base/AB/Merge \label{tab:results:rq2tests}}
\end{table}

Analysis of \Cref{tab:results:rq2tests} allows us to identify which cases identify the conflict, and which did not.
It is worth examining the results and what they mean, starting by the failing examples:

-F/FF/F: The test always fails, due to assertions that do not match the system behaviour in any state.
-P/PP/P: The test always passes, due to the correct testing of a feature not affected by the merge.

-F/FP/P: The test passes in branch B and in merge. This indicates a behaviour was introduced and remained unaltered in the merge, thus it is not a conflict in itself.
This was common for Point (CM) ChatGPT, as the prompt induced the generation of tests for distance(), which correctly calculated and asserted the euclidian distance of a given point.
They however failed to do the same when applying the distance to move(), hence the failing tests and non-identification of the conflict.

-C/CX/X or C/XC/X are usually the result of an introduction of a method in branch B or A respectively.
From these we observe the failing examples: C/CF/F or C/FC/F and C/CP/P or C/PC/P, which suffer from the same issues as F/FF/F and P/PP/P.

-R/XX/X came as the result of dictionary access, where the given key was only not present in the base commit.
Here we see R/FF/F, due to incorrect assertions and R/PF/P, which tests for the wrong branch.


The examples that show a semantic conflict is present are:

-P/PP/F: The conflict is made evident due to lost behaviour. In this case, a behaviour is always passing, except in merge where it now fails.

-F/PF/F: Same as the previous, lost behaviour shows the conflict. In this case, the behaviour was only ever present in branch A, and lost on merge.

-C/CP/F: The same pattern, but due to method introduction, only B and merge are tested.

-F/FF/P: The corollary of the the previous examples, this makes the conflict evident by showing emergent behaviour: in a merge something now happens that previously didn't happen before.
We see the parallel P/PP/F and F/FF/P in Cart(CM): while the manual explanation prompted it to check for a succesful checkout, ChatGPT's explanation prompted it to check for a failing one.
It is worth noting the *: this refers to the usage of @expected, a feature no longer supported by Junit. These tests thus required downgrading the version to run.


If we observe the results, particularly the erros found, we can see that the main issue present
is incorrect assertions. While in some cases these are understandable (such as the case of RO,
where the values to verify are hash and thus hard to know without running the code), most of the time
they were the result of simple logical or mathematical errors, things that are trivial for a human to get right
and thus also correct.
It is hard to identify why exactly it finds it so hard to get the right asserts. If we observe the cart example,
we find that tests generated for distance() were always correct. These consisted of a simple x + y addition.
However, tests generated for move() were always wrong; in turn these were just x + distance() and y + distance() additions.
Given that it shows it can correctly calculate distance() it is hard to fathom why it struggles
to use it in addition: we can perhaps suggest there is making the association between the method call and the returned value it represents.

It is worth nothing that Cart (CM) stands out: since the conflict result in the throwing of an error, asserts were easily generated as they required no numerical values.

Another highly succesful case is OAC, with 3 successes that require no edits. This is because
of the simplicity of the test: one just needs to set a value and verify that it is set:
\begin{answer}
    @Test
    public void testSetProducerNumber() {
        Event event = new Event();
        Producer producer = new Producer();
        event.setProducer(producer);
        int registrationNumber = 12345;
        int result = event.setProducerNumber(registrationNumber);
        assertEquals(registrationNumber, result);
    }
\end{answer}
In one case, we find an interesting error: the codebase contains methods named ```setResgistration'''.
For the most part they are ignored, as they are not necessary to identify the conflict. In one trial, however,
the method is called for Producer. However the ```Resgistration''' typo is corrected: ``producer.setRegistrationNumber(123);''.
Naturally, this stops code from compiling, but it is a noteworthy example of how Large-Language Models, due to their nature as 
predictive models, can fail in outlier cases, such as an unexpected typo in method namings.

It is noticeable that for the cases with compilation errors, namely the Cart examples, prompts with ChatGPT explanations
required significant more correction than prompts with manual corrections. An explanation might be that due to their more
extensive length, ChatGPT explanations prompt the generation of more complex and longer tests. However as can be seen in
\cref{tab:results:rq2tests}, both examples have comparable amount of test cases and test length does not vary much.
It is possible these are the results of outliers: the if we for example observe Cart (PM), the results are significantly weighed by
trial 3, which requires significant correction due to the way it tries to access the user:
\begin{answer}
    @Test
    public void testTotalCostWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }
\end{answer}
has an edit distance of 112, has it is corrected into:
\begin{answer}
    @Test
    public void testTotalCostWithCoupon() {
        User user = new User();
        Cart cart = new Cart(user);
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, user.balance, 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        User user = new User();
        Cart cart = new Cart(user);
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, user.balance, 0.001);
    }
\end{answer}
This same error of trying to access user from cart can be found in other ChatGPT, but it is ambiguous
whether this is somehow the result of the prompt or coincidental generations.

On the topic of compilation errors, it is worth nothing most of these were related to constructors, as the constructors
for the other classes involved in the Cart conflicts (Item and User) were not included in the prompt. It is interesting
to observe what the common error is: the introduction of extra parameters. For example, in the code of Cart only the fields
value and amount of Item are accessed. However, Item was frequently iniatilized with a constructor with 3 parameter: Name,
Value and Amount. It is fair to assume the Item class would both have a name field and that it would come first in the constructor
and it is likely the training data had many examples of this. Thus it is worth pointing out that testability of code by LLM's can be
improved by closely following norms and rule of thumb expectations and corollarily LLMs will struggle to effectively construct tests
for more unusual pieces of software.

A way to resolve this issue would be to include constructor headers in the original prompt. However one issue is the selection of which
headers: if instead of just Item and User, the Cart class had 20 other dependencies which bore no relevance in our conflict, we would be 
providing extra information in the prompt which could cause confusion. A possibly better option is feeding compilation messages back for correction,
as shown by ChatTester~\cite{kn:chattester}, possibly with the addition of corresponding headers. This corrective approach could also be utilized
to fix assert errors.

Some other issues with constructors we observed are not so obvious to solve: namely that despite knowing of available construcotrs, there's no guarantee
the correct one for the situation will be used. We can see the case of Cart (PM): due to changes in the Item class, constructor information came included
in the diff section of the prompt. This led to the creation of tests using Value,Coupon and Value format.
\begin{answer}
    public class CartTest {

    @Test
    public void testSumItemsWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        assertEquals(-50, cart.sum_items(), 0.001);
    }

    @Test
    public void testSumItemsWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        assertEquals(50, cart.sum_items(), 0.001);
    }

    @Test
    public void testTotalCostWithCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50, true); // Item with coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }

    @Test
    public void testTotalCostWithoutCoupon() {
        Cart cart = new Cart(new User());
        Item item = new Item(50); // Item without coupon
        cart.add_item(item);
        cart.checkout("loyal");
        assertEquals(-45, cart.getUser().getBalance(), 0.001);
    }
}
\end{answer}
Notably these constructors don't set the amount field, which is thus 0, rendering the items useless for any value calculations.
Rectification would require understanding this logic error and thus is liable to be more difficult to achieve.

Another error that can't be straighforwardly corrected is the testing for the wrong branches. This applies
to our parallel field example, where branch A introduces a dictionary entry for the discounts (value 0.4d) and branch B renames a previous one to the same name (value 0.25d). To find the conflict
we need to find that the entry is branch B ends up being replaced by the entry in A, that comes later in the field. Thus the only way to find the conflict is check whether the name
applies a 0.25d discount, which confirms the behaviour was lost. Checking for a 0.4d gives no information other than the introduction of branch A remains unaltered.
Unfortunately, all the tests generated verified the 0.4d behaviour, or at least tried to given the assert errors.

Other minor errors include missing asserts and invocation of non-existent getters.

Despite all the problems mentioned previously, the results are generally quite good, as most errors are relatively small and easily fixable with further prompting. Thus we
can say with some certainty that for these simple cases, where ChatGPT can identify the conflict with some degree of confidence, it can also generate appropriate unit tests.
