\chapter{Empirical methodology}\label{chap:study}

The purpose of this chapter of work was to assess the capabilities of LLM to identify and test semantic conflicts.
Thus, it was divided into two phases: an initial exploratory phase, where an unstructured exploration allowed us to develop prompts, identify characteristics of the LLM and narrow down on its abilities and limitations.
In a second phase, work was systematized, with the elaboration of research questions and metrics to evaluate results.

\section{Experimental subjects}\label{chap:study:subjects}

\begin{table}[t]
\centering
% \tabcolsep=1cm
% \renewcommand{\arraystretch}{0.90}
\begin{tabular}{@{\extracolsep{\fill}} lll} \toprule
                 & Real & Conflict \\
\midrule
Point            & No  & Change Method \\
Fabricated (RO)  & No  & Remove Override \\
Fabricated (OAC) & No  & Overload by Access Change \\
Cart (CM)        & No  & Change Method \\
Cart (PF)        & No  & Parallel Field \\
Cart (PM)        & No  & Parallel Method \\
Antlr            & Yes & Parallel Field \\
OkHttp           & Yes & Parallel Method \\
\bottomrule
\end{tabular}
\caption{Subjects used in the empirical study.\label{tab:subjects}}
\end{table}

To assess the validity of a developed solution, a collection of subjects to test must be collected. While several previous work has compiled collections of merge commits with semantic conflicts, the collection done by \citet{kn:nuno} is particularly useful, being publicly available, closely related to our own work, and also allowing us to draw direct comparisons. Most importantly, it aggregates merge instances from both \citet{kn:leuson} and \citet{kn:safemerge}, while also providing valuable information, due to the work developed, such as the specific type of conflict present and whether it was detected and correctly tested by UNSETTLE (providing us with a ``base truth''). Furthermore, it has compiled a set of fabricated conflicts, which provide simpler isolated examples that can aid us as they should be easier to detect and test.
In analyzing the work of Nuno Castanho, we found the predominant types of semantic conflicts in real scenarios where ``Parallel Changes in Field'', ``Parallel Changes in Method'' and ``Change Method'' \citet{kn:nuno}. Using a simple Cart class as a base, 3 examples were made for these scenarios.
These Cart examples would be part of the group of subjects used to answer and evaluate the research questions we defined. These were complemented by a simple Point class, two examples fabricated by Nuno Castanho of type ``Override by Access Change'' and ``Remove Override'', which had achieved good results in exploratory testing, Antlr4 and OkHttp, respectively a simple and a hard to test real-world scenario.
The collection of subjects and corresponding conflict type can be found in \Cref{tab:subjects}.

\todo{Jose: Para o leitor melhor perceber os resultados (quando estes forem
apresentados), vai ter de perceber que conflictos temos aqui.  Sugiro uma breve
descricação de (1) cada programa usado e (2) uma breve descricação do conflito
presente em cada programa.}

\section{Large language models}

\todo{\begin{itemize}
  \item ChatGPT
  \item (if time allows) GitHub Copilot Chat on VS Code / IntelliJ IDE
\end{itemize}}

While preliminary work sought to explore ChatGPT and Llama (both CodeLlama and Llama 2), hardware constraints, particularly limited GPU RAM in the machines at our disposal, meant we were unable to explore Llama.  Bing AI and Bard were also considered, but they were problematic due to very stringent message size limits, in the case of Bard, and generally worse results: Bing AI, for example, could not wait for all the information to be sent, if split in more than one message and thus generated confused responses. ChatGPT, being hosted online for free and with generous message size limits, proved to be the most reliable option. Despite this, many capabalities that could prove invaluable for this work remained locked behind a premium paywall.
The instance of ChatGPT used was the web-based ChatGPT 3.5 Turbo, with default temperature, during the first semester of 2024.

\section{Research questions}

\begin{itemize}
  \item[\textbf{RQ1:}] Can ChatGPT identify, understand, and explain, whether
  there is a semantic conflict in a merge commit?

  \item[\textbf{RQ2:}] Can ChatGPT develop unit tests to identify the introduced, altered or lost behaviour due to a semantic conflict, if given an explanation of it?
\end{itemize}

\section{Experimental procedure}

\subsection{RQ1}

In our first research question we established sought to more systematically evaluate ChatGPT's capabilities to assess and describe semantic conflicts.
We first settled on a prompt, based on the prompts that were iteratively developed on previously work.

\begin{prompt}
You are a software developer that has to assess whether there is a semantic conflict in a merge commit.  Given the base version of the class, the diff from base to a version A, the diff from base to a version B and the merged version of the class, assess whether there is a semantic conflict and explain it.

Base version:
```java
```

Diff version A and the base:
```diff
```

Diff version B and the base:
```diff
```

Merge version:
```java
```
\end{prompt}

% For each semantic conflict example, as listed previously,
The prompt was submitted three times to account for the randomess nature of the
LLM, which in turn allowed us to see a broader range of responses and avoiding
what might be one-time flukes.

As an example how one would instantiate this prompt is listed below for the
Point subject, where a conflict is introduced by the simultaneous change of
euclidean to manhattan distance followed by the usage of the same distance method
in another method.  \todo{breaks when split between 3+ pages}
\begin{prompt}
You are a software developer that has to assess whether there is a semantic conflict in a merge commit.  Given the base version of the class, the diff from base to a version A, the diff from base to a version B and the merged version of the class, assess whether there is a semantic conflict and explain it. 

Base version:
```java
public class Point {
    private double x;
    private double y;

    public Point(double x, double y) {
        this.x = x;
        this.y = y;
    }

    public double getX() {
        return x;
    }

    public double getY() {
        return y;
    }

      public void move() {
          this.x += 1;
          this.y += 1;
      }

    public double distance() {
        return Math.sqrt(Math.pow(getX(), 2)+ Math.pow(getY(), 2));
    }

    public String toString() {
        return "(" + getX() + ", " + getY() + ")";
    }
}
```

Diff version A and the base:
```diff
@@ -16,8 +16,9 @@ public class Point {
      }

      public void move() {
-         this.x += 1;
-         this.y += 1;
+         double dist = distance();
+         this.x += dist;
+         this.y += dist;
      }

      public double distance() {
```

Diff version B and the base:
```diff
@@ -21,7 +21,7 @@ public class Point {
      }

      public double distance() {
-        return Math.sqrt(Math.pow(getX(), 2)+ Math.pow(getY(), 2));
+        return Math.abs(getX())+Math.abs(getY());
      }

      public String toString() {
```

Merge version:
```java
public class Point {
    private double x;
    private double y;

    public Point(double x, double y) {
        this.x = x;
        this.y = y;
    }

    public double getX() {
        return x;
    }

    public double getY() {
        return y;
    }

      public void move() {
          double dist = distance();
          this.x += dist;
          this.y += dist;
      }

    public double distance() {
        return Math.abs(getX())+Math.abs(getY());
    }

    public String toString() {
        return "(" + getX() + ", " + getY() + ")";
    }
}
```
\end{prompt}
%
% \todo{Jose: Given the motivational example is one of the subject, I might not
% sure we should provide results in here.}
% This prompt nets us the following response from the LLM.
%
% \begin{answer}
% To assess whether there is a semantic conflict in the merge commit, let's analyze the changes introduced in versions A and B compared to the base version, and then compare them with the merged version.
% 
% ### Changes introduced in Version A:
% - The `move()` method has been modified to calculate the distance using the `distance()` method and then update the `x` and `y` coordinates accordingly.
% - This change effectively moves the point by the distance it currently is from the origin.
% 
% ### Changes introduced in Version B:
% - The `distance()` method has been modified to calculate the Manhattan distance instead of the Euclidean distance.
% - The Manhattan distance is calculated by taking the absolute value of the x and y differences.
% 
% ### Analysis of Merged Version:
% - The `move()` method from Version A has been kept in the merged version, which calculates the distance using the Euclidean distance.
% - The `distance()` method from Version B has been kept, which calculates the Manhattan distance.
% 
% ### Potential Semantic Conflict:
% - The potential conflict arises from the inconsistency between how distance is calculated in the `move()` method and the `distance()` method in the merged version.
% - While the `move()` method assumes Euclidean distance for movement, the `distance()` method calculates the Manhattan distance.
% - This inconsistency could lead to unexpected behavior, as the `move()` method might not accurately reflect the intended behavior of the `distance()` method.
% 
% ### Conclusion:
% - There is a semantic conflict in the merge commit because the merged version contains methods with conflicting assumptions about how distance should be calculated.
% - Resolving this conflict would require ensuring consistency in the distance calculation method used throughout the class, either sticking with Euclidean distance or Manhattan distance for both `move()` and `distance()` methods.
% \end{answer}

To evaluate the answers generated by ChatGPT, we established five metrics, with likert scales:
%
\begin{itemize}
  \item[\textbf{A}:] Changes between branches correctly identified (Correct description of Version A and B).
  \begin{itemize}
    \item[\textbf{0}:] Large errors in description for both branches.
    \item[\textbf{1}:] Large error in description of one branch.
    \item[\textbf{2}:] Minor insignificant errors.
    \item[\textbf{3}:] Changes correctly identified.
  \end{itemize}

  \item[\textbf{B}:] No conflict misunderstanding (Does not describe textual conflict, understand merge succeed).
  \begin{itemize}
    \item[\textbf{0}:] Understands conflict as textual.
    \item[\textbf{1}:] Understands the conflict as semantic.
  \end{itemize}

  \item[\textbf{C}:] Positive response (3 types: says conflict exists/may exist/does not exist)
  \begin{itemize}
    \item[\textbf{0}:] Denies existence of conflict
    \item[\textbf{1}:] Asserts conflict is possible
    \item[\textbf{2}:] Asserts conflict exists
  \end{itemize}

  \item[\textbf{D}:] Origin of conflict described (What code interactions lead to altered behaviour)
  \begin{itemize}
    \item[\textbf{0}:] Incorrect or non-existent explanation
    \item[\textbf{1}:] Identifies origin of conflict, with lack of clarity or imprecisions
    \item[\textbf{2}:] Identifies origin of conflict, with lack of confidence
    \item[\textbf{3}:] Identifies origin of conflict
  \end{itemize}

  \item[\textbf{E}:] Effect of conflict described (What is the result of the code interactions/expected output)
  \begin{itemize}
    \item[\textbf{0}:] Result of conflict is omitted, too vague or wrong
    \item[\textbf{1}:] Result of conflict is expressed, but with imprecisions or generically
    \item[\textbf{2}:] Possible code outputs are expressed, with little confidence
    \item[\textbf{3}:] Assertively points out expected outputs due to conflict
  \end{itemize}
\end{itemize}

% From this we derive the metrics: A=3, B=1, C=2, D=3, and E=2.
% In this case, given the correctness of the answer, all metrics have perfect scores,
% except for metric D (Effect of conflict described), due to the reduced confidence.

\subsection{RQ2}

Research Question 2 sought to evaluate chatGPT's test generation ability. Specifically, whether it could generate tests that would show the evidence the semantic conflict, given an explanation.
For example, taking the Point, if ChatGPT generates a unit test that tests the move function, we would identify the conflict if the test fails in merge (assuming the test asserts Euclidean based movement as true) or in branch B (assuming it asserts Manhattan based movement as true).
We evaluate by observing whether a functional unit test is generated and if not, the edit distance to a functioning version of one test.

\section{Threats to validity}

\todo{Describe any threat to our study, e.g., subjects, llms, our prompts, and
what have we done to mitigate them.}

Based on the guidelines reported by \citet{wohlin2012experimentation}, we have
taken all reasonable steps to mitigate the effect of potential threats, which
are described in detail in this section.

\subsection{Threats to construct validity}
%
\todo{Are associated with the correspondence between theory and observation.}

\subsection{Threats to internal validity}
%
\todo{Are associated with uncontrollable internal factors that may influence our results.}

\subsection{Threats to external validity}
%
\todo{Are associated with the generalization of the results reported.}
