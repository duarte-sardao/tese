\chapter{Empirical study}\label{chap:study}

This chapter describes the study conducted to assess the capabilities of LLMs
(in particular, ChatGPT and GitHub Copilot) at identifying, explaining,
and revealing semantic conflicts in merge commits.
% This chapter is divided into two phases: an initial exploratory phase, where an unstructured exploration allowed us to develop prompts, identify characteristics of the LLM and narrow down on its abilities and limitations.  In a second phase, work was systematized, with the elaboration of research questions and metrics to evaluate results.

\section{Experimental subjects}\label{chap:study:subjects}

\begin{table}[t]
\centering
% \tabcolsep=1cm
% \renewcommand{\arraystretch}{0.90}
\begin{tabular}{@{\extracolsep{\fill}} lll} \toprule
                 & Real Conflict & Type of Conflict \\
\midrule
Point            & No  & Change Method \\
Fabricated (RO)  & No  & Remove Override \\
Fabricated (OAC) & No  & Overload by Access Change \\
Cart (PF)        & No  & Parallel Field \\
Cart (PM)        & No  & Parallel Method \\
Cart (CM)        & No  & Change Method \\
Antlr            & Yes & Parallel Field \\
OkHttp           & Yes & Parallel Method \\
Retrofit         & Yes & Parallel and Change Method \\
\bottomrule
\end{tabular}
\caption{Subjects used in the empirical study, with information on whether subject is fabricated or extracted from a real software project, as well as the type of conflict found.\label{tab:subjects}}
\end{table}

% To assess the validity of a developed solution, a collection of subjects to test must be collected.
Previous works have established catalogs of merge commits with semantic conflicts
to ease the evaluation and comparison of any developed solution.

\citet{kn:safemerge} \todo{describe it.  how many conflicts are there, where they
come from, what type, etc.}
% analyze popular repositories, i.e., Elasticsearch, libGDX, iosched, kotlin, MPAndroidChart, okhttp, retrofit, RxJava, and the Spring Boot framework, where they find 11 positive results \todo{the results dont really matter in this subsection}.

\citet{kn:leuson} \todo{describe it.  how many conflicts are there, where they
come from, what type, etc.}

\citet{kn:nuno} \todo{describe it.  how many conflicts are there, where they
come from, what type, etc.}

\citet{kn:leuson2} established 28 merge commits with conflicts, from a total of 85.   \todo{describe it.  how many conflicts are there, where they come from, what type, etc.}

For our study we elected the subjects used in \citet{kn:nuno}'s study.  It is
publicly available, closely related to our own work, and also allowing us to
draw direct comparisons.  Most importantly, it aggregates merge instances from
both \citet{kn:leuson} and \citet{kn:safemerge}, while also providing valuable
information, due to the work developed, such as the specific type of conflict
present and whether it was detected and correctly tested by UNSETTLE~\cite{kn:nuno}
(providing us with a ``base truth'').  Furthermore, it has compiled a set of
fabricated conflicts, which provide simpler isolated examples that can aid us as
they should be easier to identify, explain, and reveal.

While analyzing \citet{kn:nuno}'s set, we found the predominant types of semantic
conflicts in real scenarios where ``Parallel Changes in Field'', ``Parallel Changes in Method'',
and ``Change Method''.  Using a simple Cart class as a base, three examples were
made for these scenarios.  These Cart examples were part of the group of subjects used to answer and evaluate the research questions we defined.  These were complemented by a simple Point class, two examples fabricated by Castanho of type ``Override by Access Change'' and ``Remove Override'', %which had achieved good results in exploratory testing\todo{not clear.  perhaps to early to say this}, 
Antlr4, OkHttp and Retrofit, respectively a simple and two hard to test real-world scenarios.
The collection of subjects and corresponding type of conflict can be found in \Cref{tab:subjects}.

% It is worth examining each subject and it's corresponding conflict, to make understanding the results easier.

\subsection{Point}

\subsubsection{Conflict Type: Change Method}

Point consists of a simple class for a two dimensional point, with fields \texttt{x} and \texttt{Y}.
Other than getters and setters, it's most relevant methods are \texttt{move} and \texttt{distance}.
Originally, the method \texttt{move} increments both \texttt{x} and \texttt{y} by 1, while \texttt{distance} computes the euclidean distance to \texttt{(0,0)}.
In branches A and B, \texttt{move} is modified to increment by \texttt{distance} while \texttt{distance} is modified to compute
the manhattan distance.  The conflict then arises when code is merged.  \texttt{move} was modified with the expectation that it would move
based on the euclidan distance, but with the merging of the other branch it will move differently.

\subsection{Fabricated}

The Fabricated subject revolves around events: ticketing, scheduling, installations, etc. The two
conflicts that make the two subjects relate to different parts of the code.

\subsubsection{Conflict Type: Remove Override}

The classes involved in the conflict are \texttt{Instalacao} and \texttt{InstalacaoAssentos}, which extends the former.
Of is the method \texttt{helpString}, which returns a string with class information.  This method is present
in both classes with different implementations (thus \texttt{InstalacaoAssentos} overrides it). In branch A,
\texttt{hashString} is introduced in \texttt{InstalacaoAssentos}: it takes \texttt{helpString} and hashes it. In branch B,
the \texttt{helpString} from \texttt{InstalacaoAssentos} is deleted. Thus, the conflict arises because in the merge,
\texttt{hashString} will be calling the \texttt{helpString} from \texttt{Instalacao} and the has values returned will be different.

\subsubsection{Conflict Type: Overload By Access Change}

The classes involved are \texttt{Event} and \texttt{Producer}, where the former has a ManyToOne relationship with the latter.
In \texttt{Producer}, there are two methods: \texttt{setResgistration(Integer)} and \texttt{setResgistration(int)}. The former sets
the producer's \texttt{registrationNumber} to the \texttt{intValue()} of the \texttt{Integer + 10}. The latter just sets the given value.
In branch A, the \texttt{int} version of the method is made private. In branch B, a \texttt{setProducerNumber(int)} function is added
to \texttt{Event}: it checks if the \texttt{Producer} is null and if not, calls \texttt{setResgistration} with the given value. In B, this will
call the \texttt{int} version of the method, as it is still public, but after the merge, the \texttt{Integer} version will be used instead,
hence the conflict.

\subsection{Cart}

Cart consists of a simple class representing a shopping cart, with auxiliary classes for users and items.
Its fields correspond to a dictionary of discounts, with string associated to doubles, a list of items in the cart
and the associated user.
It has the appropriate methods to add items to the cart, calculate the cart value, and checkout with a given discount code,
subtracting the value from the user's balance.
Cart has some pecularities which may differ from real-world classes of the same type. These include the fact that checkout
requires a discont code as an argument (there is no checkout without discount) and that items can only be added one by one, without
any way to add a collection all at once.  Notably their auxiliary classes contained only public fields, with no getters and setters, as
well as lacking expected fields such as String type ``name'' fields. These divergences from the norm came to matter when it came to
test generation.

\subsubsection{Conflict Type: Parallel Field}

The base of this conflict has some alterations compared to the other Cart versions: a more full dictionary of discounts,
a method to get the key set and a boolean method (\texttt{checkForDiscountRenewal}) that returns \texttt{true} or \texttt{false} whether the key set is less or equal than six.
The changes correspond to the removal of a discount in the first line of the field, in branch A, and the removal of a discount in the last line, on branch B.
These merge correctly but emergent behaviour arises: the key set in merge is different from both the one in A and B.
The boolean method further makes this evident.

\subsubsection{Conflict Type: Parallel Method}

The changes relate to the sum\_items() method. At the start, it just goes trough the items in the list and sums together. In branch A,
a check that certified the return is greater or equal to 0 is removed. In branch B, the boolean coupon is added to item and if an item
is of type coupon, its value is removed rather than added. There is emergent behaviour in the merge, as coupons now allow carts of negative
value to exist.

\subsubsection{Conflict Type: Change Method}

The conflict arises due to the handling of checkout. In branch A, a check is added where if a user is of type admin,
\texttt{total\_cost} method returns 0. In branch B, a check is added in the \texttt{checkout} method: if \texttt{total\_cost} is 0, a runtime exception is thrown.
Naturally these changes conflict in the merge.

\subsection{Antlr}

\subsubsection{Conflict Type: Parallel Field}

Antlr is much like Cart (Parallel Field), but with additions instead of removals: it contains a String array called \texttt{python2Keywords}, which is returned as a set by the \texttt{getBadWords} method.
In branch A, the string ``del'' is added and in branch B, the string ``return'' is added.
Thus in the merged version we will have a emergent version of the \texttt{BadWords}
set, with different length and contents compared to both previous versions.

\subsection{OkHttp}

\subsubsection{Conflict Type: Parallel Method}

OkHttp provides an HTTP client. The class OkHttpClient configures and creates HTTP connections. This process involves
the \texttt{copyWithDefaults} method, which creates a shallow copy, using system defaults for fields which are not set. Both branches
modify this method, one by changing how \texttt{failedRoutes} are handled and adding a field for transports, while the other replaces
the default hostname verifier with a custom implementation, \texttt{OkHostnameVerifier}. As a result, the copy returned on merge will
be different from both branches, and this difference will trickle down into the initialization of \texttt{HttpURLConnectionImpl}, which
as the copy as an argument.

\subsection{Retrofit}

\subsubsection{Conflict Type: Parallel and Change Method}

Another HTTP client. Relevant to the conflict is the class \texttt{RequestBuilder}, which builds HTTP requests from Java method invocations. The methods modified
in the conflict are \texttt{build}, which returns a \texttt{Request}, and the method \texttt{buildRelativeUrl}, which is invoked inside of \texttt{build}.
Branch A only modified how request queries are handled, by adding the appropriate \texttt{?} to the url when one is present.
Branch B adds several checks for null arguments for request query arguments, path parameters, form fields and multipart parts, throwing an exception or ignoring them as adequate.
Both of these chances are integrated and as a result, the behaviour of the construction of a \texttt{Request} is different in the merge.

\section{Large Language Models (LLMs)}

While preliminary work sought to explore ChatGPT~\cite{kn:gpt} and Llama~\cite{kn:llama} (both CodeLlama and Llama 2), hardware constraints, particularly limited GPU RAM in the machines at our disposal, meant we were unable to explore Llama.  Bing AI~\cite{kn:bingAI} and Bard~\cite{kn:bard} were also considered, but they were problematic due to very stringent message size limits, in the case of Bard, and generally worse results: Bing AI, for example, could not wait for all the information to be sent, if split in more than one message and thus generated confused responses. ChatGPT, being hosted online for free and with generous message size limits, proved to be the most reliable option. Despite this, many capabalities that could prove invaluable for this work remained locked behind a premium paywall.
The instance of ChatGPT used was the web-based ChatGPT 3.5 Turbo, with default temperature, during the first semester of 2024.
The version of GitHub Copilot was version 1.5.6.5692.

\section{Research questions}

\begin{itemize}
  \item[\textbf{RQ1:}] Can ChatGPT identify, understand, and explain, whether
  there is a semantic conflict in a merge commit?

  \item[\textbf{RQ2:}] Given an explanation of a semantic conflict in a merge
  commit, can ChatGPT generate unit test case that are able to identify it?
  % the emergent behaviour of to a semantic conflict, if given an explanation of it?

  \item[\textbf{RQ3:}] Can state-of-the-art prompts lead ChatGPT to generate to
  unit test cases that are able to identify semantic conflicts?  How do they
  compare to the prompt used in RQ2?
\end{itemize}

\section{Experimental procedure and metrics}

\subsection{RQ1}

In our first research question we established sought to more systematically evaluate ChatGPT's capabilities to assess and describe semantic conflicts.
We first settled on a prompt, based on the prompts that were iteratively developed on previously work.

\begin{prompt}
You are a software developer that has to assess whether there is a semantic conflict in a merge commit.  Given the base version of the class, the diff from base to a version A, the diff from base to a version B and the merged version of the class, assess whether there is a semantic conflict and explain it.

Base version:
```java
```

Diff version A and the base:
```diff
```

Diff version B and the base:
```diff
```

Merge version:
```java
```
\end{prompt}

% For each semantic conflict example, as listed previously,
The prompt was submitted three times to account for the randomess nature of the
LLM, which in turn allowed us to see a broader range of responses and avoiding
what might be one-time flukes.

As an example how one would instantiate this prompt is listed below for the
Point subject, where a conflict is introduced by the simultaneous change of
euclidean to manhattan distance followed by the usage of the same distance method
in another method.
\begin{prompt}
You are a software developer that has to assess whether there is a semantic conflict in a merge commit.  Given the base version of the class, the diff from base to a version A, the diff from base to a version B and the merged version of the class, assess whether there is a semantic conflict and explain it.

Base version:
```java
public class Point {
    private double x;
    private double y;

    public Point(double x, double y) {
        this.x = x;
        this.y = y;
    }

    public double getX() {
        return x;
    }

    public double getY() {
        return y;
    }

      public void move() {
          this.x += 1;
          this.y += 1;
      }

    public double distance() {
        return Math.sqrt(Math.pow(getX(), 2)+ Math.pow(getY(), 2));
    }

    public String toString() {
        return "(" + getX() + ", " + getY() + ")";
    }
}
```

Diff version A and the base:
```diff
@@ -16,8 +16,9 @@ public class Point {
      }

      public void move() {
-         this.x += 1;
-         this.y += 1;
+         double dist = distance();
+         this.x += dist;
+         this.y += dist;
      }

      public double distance() {
```

Diff version B and the base:
```diff
@@ -21,7 +21,7 @@ public class Point {
      }

      public double distance() {
-        return Math.sqrt(Math.pow(getX(), 2)+ Math.pow(getY(), 2));
+        return Math.abs(getX())+Math.abs(getY());
      }

      public String toString() {
```

Merge version:
```java
public class Point {
    private double x;
    private double y;

    public Point(double x, double y) {
        this.x = x;
        this.y = y;
    }

    public double getX() {
        return x;
    }

    public double getY() {
        return y;
    }

      public void move() {
          double dist = distance();
          this.x += dist;
          this.y += dist;
      }

    public double distance() {
        return Math.abs(getX())+Math.abs(getY());
    }

    public String toString() {
        return "(" + getX() + ", " + getY() + ")";
    }
}
```
\end{prompt}
%
% \todo{Jose: Given the motivational example is one of the subject, I might not
% sure we should provide results in here.}
% This prompt nets us the following response from the LLM.
%
% \begin{response}
% To assess whether there is a semantic conflict in the merge commit, let's analyze the changes introduced in versions A and B compared to the base version, and then compare them with the merged version.
%
% ### Changes introduced in Version A:
% - The `move()` method has been modified to calculate the distance using the `distance()` method and then update the `x` and `y` coordinates accordingly.
% - This change effectively moves the point by the distance it currently is from the origin.
%
% ### Changes introduced in Version B:
% - The `distance()` method has been modified to calculate the Manhattan distance instead of the Euclidean distance.
% - The Manhattan distance is calculated by taking the absolute value of the x and y differences.
%
% ### Analysis of Merged Version:
% - The `move()` method from Version A has been kept in the merged version, which calculates the distance using the Euclidean distance.
% - The `distance()` method from Version B has been kept, which calculates the Manhattan distance.
%
% ### Potential Semantic Conflict:
% - The potential conflict arises from the inconsistency between how distance is calculated in the `move()` method and the `distance()` method in the merged version.
% - While the `move()` method assumes Euclidean distance for movement, the `distance()` method calculates the Manhattan distance.
% - This inconsistency could lead to unexpected behavior, as the `move()` method might not accurately reflect the intended behavior of the `distance()` method.
%
% ### Conclusion:
% - There is a semantic conflict in the merge commit because the merged version contains methods with conflicting assumptions about how distance should be calculated.
% - Resolving this conflict would require ensuring consistency in the distance calculation method used throughout the class, either sticking with Euclidean distance or Manhattan distance for both `move()` and `distance()` methods.
% \end{response}

To evaluate the answers generated by ChatGPT, we established five metrics (A--E), each with likert scales.
%
\begin{itemize}
  \item[\textbf{A}:] Changes between branches correctly identified (Correct description of Version A and B).
  \begin{itemize}
    \item[\textbf{0}:] Large errors in description for both branches.
    \item[\textbf{1}:] Large error in description of one branch.
    \item[\textbf{2}:] Minor insignificant errors.
    \item[\textbf{3}:] Changes correctly identified.
  \end{itemize}

  \item[\textbf{B}:] No conflict misunderstanding (Does not describe textual conflict, understand merge succeed).
  \begin{itemize}
    \item[\textbf{0}:] Understands conflict as textual.
    \item[\textbf{1}:] Understands the conflict as semantic.
  \end{itemize}

  \item[\textbf{C}:] Positive response (3 types: says conflict exists/may exist/does not exist)
  \begin{itemize}
    \item[\textbf{0}:] Denies existence of conflict
    \item[\textbf{1}:] Asserts conflict is possible
    \item[\textbf{2}:] Asserts conflict exists
  \end{itemize}

  \item[\textbf{D}:] Origin of conflict described (What code interactions lead to altered behaviour)
  \begin{itemize}
    \item[\textbf{0}:] Incorrect or non-existent explanation
    \item[\textbf{1}:] Identifies origin of conflict, with lack of clarity or imprecisions
    \item[\textbf{2}:] Identifies origin of conflict, with lack of confidence
    \item[\textbf{3}:] Identifies origin of conflict
  \end{itemize}

  \item[\textbf{E}:] Effect of conflict described (What is the result of the code interactions/expected output)
  \begin{itemize}
    \item[\textbf{0}:] Result of conflict is omitted, too vague or wrong
    \item[\textbf{1}:] Result of conflict is expressed, but with imprecisions or generically
    \item[\textbf{2}:] Possible code outputs are expressed, with little confidence
    \item[\textbf{3}:] Assertively points out expected outputs due to conflict
  \end{itemize}
\end{itemize}

% From this we derive the metrics: A=3, B=1, C=2, D=3, and E=2.
% In this case, given the correctness of the answer, all metrics have perfect scores,
% except for metric D (Effect of conflict described), due to the reduced confidence.

\subsection{RQ2}

In this research question we sought to evaluate ChatGPT's test generation ability. Specifically, whether it could generate tests that would show the evidence the semantic conflict, given an explanation of the conflict.
For instance, taking the Point as an example, if ChatGPT generates a unit test that exercises the \texttt{move} method, we would identify the conflict by generating a test that passes in the merge but fails in branch A and branch B, corresponding to the definition of emergent behaviour (see \citet{kn:nuno}, Chapter 3.1 for more information).
In this case, the test run in the different versions would show us the altered behaviour of \texttt{move} due to the conflict.

It is known that the generation of incorrect code, whether semantically or syntatically incorrect is a frequent issue \todo{where?}. \Citet{kn:wheredofail} showed the extent to which this applies across a varied range of LLMs,
indicating this is a general issue and not specific to any individual model. As regards ChatGPT (and test generation) specifically, \Citet{kn:chattester} found that only 42.1\% of generated tests compiled and a further
24.8\% passed. Of those that did not pass, they failed because of faulty generation, not due to any preexisting bug in the code.
\Citet{kn:chattester} found the most common errors to involve symbol resolution, due to the usage of undeclared classes (which far overshadowed any other error), methods or variables. Other less frequent errors were accesses on private classes and the usage
of invalid types for operations, constructor or method parameters.
Errors during execution mainly fell on assertion errors, with a minor amount of runtime exceptions.

Taking this into account, to evaluate the tests we generated, corrections were made. These involved the smallest possible corrections to make the code compile and pass in merge, by correcting syntatic errors and failing asserts.
For example, given the following generated test:
\begin{response}
@Test
public void testSetProducerNumber() {
    Event event = new Event();
    Producer producer = new Producer();
    producer.setRegistrationNumber(123); // Set registration number
    event.setProducer(producer);
    int expected = 123;
    int result = event.setProducerNumber(456); // Try to set different registration number
    assertEquals(expected, result);
}
\end{response}
We applied the corrections and achieved this test:
\begin{response}
@Test
public void testSetProducerNumber() {
    Event event = new Event();
    Producer producer = new Producer();
    producer.setResgistrationNumber(123); // Set registration number
    event.setProducer(producer);
    int expected = 466;
    int result = event.setProducerNumber(456); // Try to set different registration number
    assertEquals(expected, result);
}
\end{response}
Here, we fixed a method call, which in correcting a typo in the code tried to call a non-existent function, and ammended the value to be used in the assertion.
Other common edits were ammending constructor calls with extra parameters or replacing getter calls for getters which did not exist with direct accesses.

From this we calculated the edit distance, which corresponds to the Levenshtein Distance, excluding comments.
In this case, it was four.

Where the test was incorrect to such an extent correction would require a major rework, corrections were not applied. This helped to distinguish logically sound tests
that suffered from minor easy-to-fix compilation errors from broken tests which were not useful for our purposes. Furthermore, these minor changes were liable to be automated,
either through scripting or through the usage of the LLM itself, as shown by \Citet{kn:chattester}.

We evaluated by observing whether a syntatically correct and passing unit test was generated and if not, the edit distance to a functioning version of one test. For failing tests, we also noted which error led to the failure of compilation or test. The types were:
\begin{itemize}
  \item Assert: The logic of the test makes sense, but the value of the assertion is incorrect.
  \item Constructor: The test fails to compile because an Object is incorrectly constructed.
  \item Branch: The test is functional, but does not make the conflict evident because it is testing for the wrong branch.
  \item Miss: Missing asserts, or existing assert does not target the conflict.
  \item Other: Assorted errors in test logic or method calls.
\end{itemize}
These allowed us to further evaluate how the prompting could be improved and if these errors could be fixed.

The basic structure of the prompt follows:
\begin{prompt}
You are a software developer that has to write a set of JUnit test cases to trigger a semantic conflict that has been identified in a merge commit.
Given the base version of the class, the diff from base to a version A, the diff from base to a version B and the merged version of the class, and an explanation of the existing semantic conflict, generate the appropriate JUnit test cases that trigger the conflict.

Base version:
```java
```

Diff version A and the base:
```diff
```

Diff version B and the base:
```diff
```

Merge version:
```java
```

Explanation of the semantic conflict:
Generate just code with necessary imports. No explanation needed.
\end{prompt}
Two versions of the prompt were run, each with three trials: in the first, the explanation was manually written, whereas in the highest performance answer from ChatGPT in RQ1 was used.
This way it could be evaluated to what extent this influence test generation results.

Testing of the subjects was done on the basis of identifying emergent behaviour: this involves the generation of unit tests that
pass in merge and fail in the parent branches (or one branch fails to compile)~\citep{kn:nuno}. This identifies the conflict by
asserting the behaviour present in merge that was not present before, ie, emerges.

\subsection{RQ3}

In this research question we sought to assess the relevance of our developed work by assessing the generative abilities of previous state-of-the-art prompts, which just generate tests for methods, with no conflict information.
We also used GitHub Copilot, which allowed us to consider to what extent the usage of another LLM affects test generation.

We selected four prompts:
\begin{itemize}
  \item Prompt \#1: From ``No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation'' by ~\citet{kn:chattester}.
  \item Prompt \#2: From ``Using Large Language Models to Generate JUnit Tests: An Empirical Study'' by ~\citet{kn:siddiq2023empirical}.
  \item Prompt \#3: From ``ChatUniTest: A Framework for LLM-Based Test Generation'' by~\citet{kn:chatunitest}.
  \item Prompt \#4: Second part of prompt 2, the test skeleton, but instead used GitHub Copilot for completion.
\end{itemize}
%
We then ran three trials were run for Prompt \#1 and Prompt \#3, whereas Prompt \#2 and \#4 only had one as they showed no variation (due to no temperature).
We evaluated the prompts in two steps: in a first step we observed if the generated tests compiled and in case they did, if the test generated were passing.
In a second step, we examined the test logic itself to see if the tests can potentially identify the semantic conflict, if the same type of simple corrections applied in RQ2 were applied.

\section{Threats to validity}

Based on the guidelines reported by \citet{wohlin2012experimentation}, we have
taken all reasonable steps to mitigate the effect of potential threats, which
are described in detail in this section.

\subsection{Threats to construct validity}

We recognize the threat to construct validity played by the prompts we developed, as it is possible they were not appropriate for the research questions
we defined. We mitigated this by exploring several techniques, both by studying state-of-art prompting techniques as well as experimenting with focus in our use case in particular, before settling on the prompts we defined, but other techniques might have yielded better
results.

\subsection{Threats to internal validity}

Internal validity may have influenced by human errors, such as minor errors in prompts or mistakes in the collection and computation of results.
We mitigated this with frequent internal revisions. % and several mistakes were caught.

\subsection{Threats to external validity}

A main threat to external validity was our reliance to the LLMs used, i.e., ChatGPT-3.5 Turbo and GitHub Copilot. While we made efforts to investigate other LLMs
we were constrained by hardware and financial limitations. Thus, these results are not necessarily generalizable and further study would be required to see how other models compare.  Nevertheless, these two are state-of-art solutions and existing research
does not seem to indicate any other existing LLM is significantly better.
