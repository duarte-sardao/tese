\chapter{Conclusion \& Future Work}\label{chap:conclusion}

\section{Conclusion}

In this thesis, we presented and explored the problem of software merges, more specifically, the difficulty in establishing an automated method to identify and reveal semantic conflicts that arise.

We explored the related works that have provided avenues of study and structures to guide the development of our solution. Specifically, we explored previous work on test generation to identify semantic conflicts, the state of the art of automated test generation, and the usage of Large Language Models (LLMs), in particular ChatGPT, for test generation, with specific focus on the importance of appropriate prompting as well as output correction.

% By establishing the three research question we established, we could investigate the viability of using LLMs in this way. From this we can say:
Our empirical study revealed that:
%
\begin{itemize}
  \item %\textbf{RQ1:} We established that
  ChatGPT has some capability to identify semantic conflicts. One of the obstacles found was understanding:
conflicts which did not create obvious code faults (such as parallel addition to lists) were not seen as conflicts. 

  \item %\textbf{RQ2:} We found that
  Despite many small errors with the test setup and asserts, when an appropriate explanation was given, ChatGPT could generate
tests that appropriately verified the required behaviours. We also found no significant difference between human written and ChatGPT written explanations. The main obstacle
was the complexity of some subjects, particularly the real-world ones, where ChatGPT struggled to generate adequate tests.

  \item %\textbf{RQ3:} By investigating state of the art prompts, we observed that
  While ChatGPT fed with state of the art prompts could still generate tests that adequately revealed the conflict (after correction),
this came with downsides, due to the lack of focus provided by the detailed explanation: there was higher variability as it was not guaranteed the right setup would be attempted.
For example, in tests that required a User class to have the admin field set to true, nothing in the prompt indicated this and while generating many tests will increase the chances
of this field being set, more tests necessarily imply more time and work.
\end{itemize}

Forward, future work may apply this knowledge in developing a solution that can hopefully aid developer workflows by identifying and generating tests for semantic conflicts.

\section{Future Work}

During the course of our research we found many possible future avenues of research based both on our own work and existing research. Future work which explores these could improve
the quality of our prompting techniques and eventually developed a solution to automatically generate appropriate unit tests when a semantic merge conflict is detected in a commit.

Future improvements to prompting include:
%
\begin{itemize}
\item \textbf{Improving understanding of semantic conflicts:} As the LLM demonstrated an association between unintended software faults and conflicts, subtle conflicts were ignored.
Possible work could focus on providing k-shot learning to improve this capability, or avoid the usage of ``conflict'' in favour of more direct or neutral terms, such as prompting
for the existence of lost or emergent behaviour.

\item \textbf{Prompt automation and picking necessary information:} For our purposes, prompts were manually written. To fit within token limits and to avoid confusing the LLM, informed was cropped to
the necessary: only the class were the main affected method was present was given. In the case of real-world scenarios, diffs were also cropped, as they included a lot of extraneous information.
In an automated solution, investigation needs to be done on identifying the necessary elements required for the prompt and how to automatically extract them.

\item \textbf{Test generation with context:} Throughout all our test generations, compilations tended to fail due to small errors, particularly with constructors, field access and method calls.
Providing context by prompting with an existing suite has shown to be a reliable solution and would also aid in following stylistically conventions a project may be using.
This could also be helpful with other issues, such as using mocking and more complex techniques when and only when it is necessary.

\item \textbf{Integration with Changes-Matcher~\cite{kn:nuno}:} Changes-Matcher detects semantic conflicts by comparing merges to common patterns, such as Change Method, Parallel Field and others
seen throughout our work. Integrating these, possibly with a prompt to explain the specific type of conflict present could be helpful in conflict explanation.
Furthermore, Changes-Matcher outputs a DSL which provides information on how to test the conflict (indicating which methods should be called directly and indirectly). This could possibly
also improve test generation.

\item \textbf{Application of testability transformations:} Previous research has applied testability transformations, such as making methods and fields public or creating empty constructors~\cite{kn:leuson}.
As we noted the difficulty LLMs had with aspects such as private methods, application of these transformations could make succesful testing much more likely.
\end{itemize}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/tool.pdf}
    \caption{Pipeline of a future tool that would effectively explore the usage
    of LLMs to reveal semantic conflicts.}
    \label{fig:tool}
\end{figure}

Building upon this work, it should be possible to develop a fully automated tool to identify the conflict and from there generate appropriate tests,
which \Cref{fig:tool} describes.
