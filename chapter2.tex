\chapter{Related Work} \label{chap:sota}

\section{Introduction}

 The following chapters introduces the two main topics dealt with in this dissertation (merge conflicts and the usage of large language models for software verification and test generation). They explore related work and how we can build on it to develop our approach.

\section{Merge Conflicts and Semantic Conflicts}

The study of merge techniques and conflicts has a long history, likely even predating the specific terminology itself. <- this is awful

\subsection{Detecting Semantic Conflicts}

Several related work exists, seeking to identify methods that can more systemically identify adulterated behaviour arising from semantic conflicts.

\subsubsection{Unit Tests}

leuson, nuno

\subsubsection{Automated Behaviour Change Detection}

Also by Leuson, we find an attempt at identifying cases of semantic conflict by applying automated behaviour change detection. ~\citep{kn:leuson} <-- summarize



\section{Test Generation with LLM's}

The recent explosion in complexity and popularity of LLM's has suscitated developer interest in their abilities with regards to accelerate and automate software engineering. Angela Fan et al identified that by 2023 3\% of pre-prints were related to Large Language Models and 11\% of those related to their use in software engineering.\cite{kn:angela} Particularly relevant is their ability to generate tests, with an expectation that they could achieve better coverage, correctness and readability than previous techniques of automated test generation.\cite{kn:junjiewang}
In comparison to traditional suites for automated test generation, such as EvoSuite, Palus, Randoop, and JTExpert, ChatGPT has shown, given right tuning of temperature settings, to show equivalent robustness.\cite{kn:gptunitbra}
The maximization of ChatGPT's abilities with regards to test generation has been explored: techniques such as prompting the LLM for a explanation of what the code is intending to do \cite{kn:nuances} and feeding error messages from codes that fail to compile or execute has intended back to the LLM for correction \cite{kn:chattester} have shown an amazing capacity for test generation, given the right prompting.


\section{Conclusions}

Aliquam erat volutpat. Nunc pede ipsum, porttitor eu, bibendum non,
bibendum nec, nisl. Maecenas eget mauris. Nullam pulvinar. Curabitur
rutrum commodo est. Nam sapien pede, interdum eu, accumsan ultrices,
venenatis sit amet, tellus. Praesent ac ante bibendum enim varius
suscipit. Donec enim. Proin nisi. Quisque libero turpis, varius ut,
elementum vel, pulvinar sed, nunc. 
